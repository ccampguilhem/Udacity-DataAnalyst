{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap data\n",
    "[CÃ©dric Campguilhem](https://github.com/ccampguilhem/Udacity-DataAnalyst), August 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Top\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Project organisation](#Organisation)\n",
    "- [Map area selection](#Area selection)\n",
    "- [XML data structure](#XML data structure)\n",
    "- [Data quality audit](#Data quality)\n",
    "    - [Validity](#Data validity)\n",
    "    - [Accuracy](#Data accuracy)\n",
    "    - [Completeness](#Data completeness)\n",
    "    - [Consistency](#Data consistency)\n",
    "    - [Uniformity](#Data uniformity)\n",
    "    - [Conclusion](#Audit conclusion)\n",
    "- [Data cleaning](#Data cleaning)\n",
    "    - [Method](#Method)\n",
    "    - [Converting to dictionnary-like structure](#Converting to dictionnary-like structure)\n",
    "    - [Cleaning accuracy issues](#Cleaning accuracy issues)\n",
    "    - [Cleaning completeness issues](#Cleaning completeness issues)\n",
    "    - [Cleaning consistency issues](#Cleaning consistency issues)\n",
    "    - [Cleaning uniformities issues](#Cleaning uniformities issues)\n",
    "    - [Conclusion](#Cleaning conclusion)\n",
    "- [Data export](#Data export)\n",
    "    - [To JSON and MongoDB](#JSON MongoDB)\n",
    "    - [To csv and SQLite](#csv SQLite)\n",
    "- [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Introduction\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction *[top](#Top)*\n",
    "\n",
    "This project is related to Data Wrangling with MongoDB course for Udacity Data Analyst Nanodegree program.\n",
    "The purpose of this project is to:\n",
    "\n",
    "- Collect data from [OpenStreetMap](https://www.openstreetmap.org) web services.\n",
    "- Clean the data by fixing few issues introduced by users.\n",
    "- Store the dataset in a database to make any further analysis easier.\n",
    "\n",
    "OpenStreetMap is open data, licensed under the Open Data Commons Open Database License (ODbL) by the OpenStreetMap Foundation (OSMF). \n",
    "\n",
    "This project cover various aspects of data wrangling phase:\n",
    "- **screen scraping** with [Requests](http://requests.readthedocs.io/en/master/), an http Python library for making requests on web services,\n",
    "- **parsing** XML files with iterative and SAX parsers with Python standard library [xml.etree.ElementTree](https://docs.python.org/2/library/xml.etree.elementtree.html?highlight=iterparse#module-xml.etree.ElementTree) and [xml.sax](https://docs.python.org/2/library/xml.sax.html),\n",
    "- **auditing** (validity, accuracy, completeness, consistency and uniformity) and **cleaning** data with Python,\n",
    "    - validity: does data conform to a schema ?\n",
    "    - accuracy: does data conform to gold standard (a dataset we trust) ?\n",
    "    - completeness: do we have all records ?\n",
    "    - consistency: is dataset providing contradictory information ?\n",
    "    - uniformity: are all data provided in the same units ?\n",
    "- **storing** data into SQL database (SQLite) with Python [sqlite3](https://docs.python.org/2/library/sqlite3.html) module and [MongoDG](https://www.mongodb.com/) no-SQL database.\n",
    "- exploring dataset **statistics** as per project requirements (size of the file, number of unique users, number of nodes and ways, number of chosen type of nodes, like cafes, shops etc.)\n",
    "\n",
    "The storing step will make use of [csv](https://docs.python.org/2/library/csv.html?highlight=csv#module-csv) and [json](https://docs.python.org/2/library/json.html?highlight=json#module-json) formats respectively for SQL and MongoDB exports.\n",
    "\n",
    "I am already a bit familiar with SQL but I will also provide SQL output in addition to MongoDB output for the cleaned dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Project organisation'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project organisation *[top](#Top)*\n",
    "\n",
    "The project is decomposed in the following manner:\n",
    "\n",
    "- This notebook (data_wrangling.ipynb) contains top-level code as well as results and report.\n",
    "- The [data_wrangling.py](./data_wrangling.py) file is a Python export of this notebook.\n",
    "- The [data_wrangling.html](./data_wrangling.html) file is a html export of this notebook.\n",
    "- The [environment.yml](./environment.yml) file contains the anaconda environment I used for this project.\n",
    "- The [handler.py](./handler.py) module contains a class used as content handler for SAX XML parser.\n",
    "- The [utils.py](./utils.py) module contains functions used by audit classes.\n",
    "- The [validity_audit.py](./validity_audit.py) module contains a callback class for validity audit.\n",
    "- The [accuracy_audit.py](./accuracy_audit.py) module contains a callback class for accuracy audit.\n",
    "- The [completeness_audit.py](./completeness_audit.py) module contains a callback class for completeness audit.\n",
    "- The [consistency_audit.py](./consistency_audit.py) module contains a callback class for consistency audit.\n",
    "- The [uniformity_audit.py](./uniformity_audit.py) module contains a callback class for uniformity audit.\n",
    "- The [dictionnary_export.py](./dictionnary_export.py) module contains a callback class to export data to a dicitonnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable auto-reload of modules, will help as we have a lot of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Area selection\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map area selection *[top](#Top)*\n",
    "\n",
    "If you don't want to have details on how the data from OpenStreetMap is retrieved, you can skip this section. At the end of the processing, you should have a *data.osm* file in the same directory than this notebook.\n",
    "\n",
    "I have made the map area selection dynamic. By configuring few variables, a different map area may be extracted from OpenStreetMap. Some pre-selections are available:\n",
    "\n",
    "| Pre-selection | Description               | Usage               | File size (bytes) | OpenStreetMap link |\n",
    "|:------------- |:------------------------- |:------------------- | -----------------:|:------------------ |\n",
    "| Tournefeuille | The city I live in        | Project review      | 103 143 437       | [link](https://www.openstreetmap.org/relation/35735)\n",
    "| City center   | Tournefeuille city center | Testing, debugging  | 583 419           | [link](https://www.openstreetmap.org/export#map=14/43.5848/1.3516)\n",
    "| Toulouse      | Toulouse and surroundings | Benchmark           | 1 271 859 210     | [link](https://www.openstreetmap.org/search?query=toulouse#map=11/43.6047/1.4442)\n",
    "\n",
    "The box variables are in the following order (south-west to north-east):\n",
    "\n",
    "- minimum latitude\n",
    "- minimum longitude\n",
    "- maximum latitude\n",
    "- maximum longitude\n",
    "\n",
    "**Note: ** The data cleaning provided in this project works for French area, if you select a non-french area no data cleaning will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SELECTION = \"PRESELECTED\" #Update the PRESELECTION variable\n",
    "#SELECTION = \"USER\" #Update the USER_SELECTION with the box you want\n",
    "#SELECTION = \"CACHE\" #Use any data file present in directory\n",
    "USER_SELECTION = (43.5799, 1.3434, 43.5838, 1.3496)\n",
    "PRESELECTIONS = {\"Tournefeuille\": (43.5475, 1.2767, 43.6019, 1.3909),\n",
    "                 \"City center\": (43.5799, 1.3434, 43.5838, 1.3496),\n",
    "                 \"Toulouse\": (43.3871, 0.9874, 43.8221, 1.9006)}\n",
    "PRESELECTION = \"Tournefeuille\"\n",
    "TEMPLATE = \\\n",
    "\"\"\"\n",
    "(\n",
    "   node({},{},{},{});\n",
    "   <;\n",
    ");\n",
    "out meta;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used screen scrapping techniques presented throught the course to extract data from OpenStreetMap:\n",
    "\n",
    "- I use the Overpass API (http://wiki.openstreetmap.org/wiki/Overpass_API)\n",
    "- The query form (http://overpass-api.de/query_form.html) sends a POST request to http://overpass-api.de/api/interpreter\n",
    "- From the api/interpreter we can just make a GET request which takes a data parameter containing the box selection:\n",
    "\n",
    "```\n",
    "(\n",
    "   node(51.249,7.148,51.251,7.152);\n",
    "   <;\n",
    ");\n",
    "out meta;\n",
    "```\n",
    "\n",
    "The idea is to send a http GET request using [Requests](http://requests.readthedocs.io/en/master/) and collect results in a stream. This is because the data we get from the request may be huge and may not fit into memory.\n",
    "\n",
    "The following method `download_map_area` enables to download map area data and store it in a *data.osm* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_map_area():\n",
    "    \"\"\"\n",
    "    Download the map area in a file named data.osm.\n",
    "    \n",
    "    This function takes into account the following global variables: SELECTION, USER_SELECTION, PRESELECTIONS, \n",
    "    PRESELECTION and TEMPLATE\n",
    "    \n",
    "    If a http request is made, the response status code is returned, otherwise None in returned.\n",
    "    If SELECTION is set to CACHE and no file is present an exception is raised.\n",
    "    \n",
    "    - raise ValueError: if SELECTION=CACHE and there is no cached file\n",
    "    - raise ValueError: if SELECTION is not [PRESELECTED, USER, CACHE]\n",
    "    - raise NameError if either of SELECTION, PRESELECTION, PRESELECTIONS, USER_SELECTION or TEMPLATE does not exist.\n",
    "    - return: tuple:\n",
    "        - status code or None\n",
    "        - path to dataset\n",
    "        - dataset file size (in bytes)\n",
    "    \"\"\"\n",
    "    filename = \"data.osm\"\n",
    "    if SELECTION == \"CACHE\":\n",
    "        if not os.path.exists(filename):\n",
    "            raise ValueError(\"Cannot use SELECTION=CACHE if no {} file exists.\".format(filename))\n",
    "        else:\n",
    "            return None, filename, os.path.getsize(filename)\n",
    "    elif SELECTION == \"PRESELECTED\":\n",
    "        data = TEMPLATE.format(*PRESELECTIONS[PRESELECTION])\n",
    "    elif SELECTION == \"USER\":\n",
    "        data = TEMPLATE.format(*USER_SELECTION)\n",
    "    else:\n",
    "        raise ValueError(\"SELECTION=\")\n",
    "        \n",
    "    #Get XML data\n",
    "    r = requests.get('http://overpass-api.de/api/interpreter', params={\"data\": data}, stream=True)\n",
    "    with open(filename, 'wb') as fobj:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                fobj.write(chunk)\n",
    "    return r.status_code, filename, os.path.getsize(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 348 ms, sys: 144 ms, total: 492 ms\n",
      "Wall time: 7.23 s\n",
      "The file data.osm has been successfully downloaded. Its size is 103241495 bytes.\n"
     ]
    }
   ],
   "source": [
    "#Download dataset\n",
    "%time status_code, dataset_path, dataset_size = download_map_area()\n",
    "if status_code is None:\n",
    "    print \"The file {} is re-used from a previous download. Its size is {} bytes.\".format(dataset_path, dataset_size)\n",
    "elif status_code == 200:\n",
    "    print \"The file {} has been successfully downloaded. Its size is {} bytes.\".format(dataset_path, dataset_size)\n",
    "else:\n",
    "    print \"An error occured while downloading the file. Http status code is {}.\".format(status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"XML data structure\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML data structure *[top](#Top)*\n",
    "\n",
    "In the previous section, we have downloaded a dataset from OpenStreetMap web service. The XML file retrieved this way is stored in the file named *data.osm*.\n",
    "\n",
    "In this section we are going to familiarize with the dataset to understand how it's built. As dataset may be a very large file (depending on the map area extracted) we are going to use an iterative parser that does not need to load the entire document in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the XML library\n",
    "import xml.etree.cElementTree as et\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 17264,\n",
      " 'meta': 1,\n",
      " 'nd': 602009,\n",
      " 'node': 428787,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 469,\n",
      " 'tag': 218929,\n",
      " 'way': 71960}\n"
     ]
    }
   ],
   "source": [
    "#Iterative parsing\n",
    "element_tags = Counter()\n",
    "for (event, elem) in et.iterparse(dataset_path):\n",
    "    element_tags[elem.tag] += 1\n",
    "pprint(dict(element_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenStreetMap, data is structured this way:\n",
    "- A **node** is a location in space defined by its latitude and longitude. It might indicate a standalone point and/or can be used to define shape of a way.\n",
    "- A **way** can be either a polyline to represent roads, rivers... or a closed polygon to delimit areas (buildings, parks...).\n",
    "- A **nd** is used within way to reference nodes.\n",
    "- A **relation** can be defined from **member** nodes and ways to represent routes, bigger area such as regions or city boundaries.\n",
    "- A **member** is a subpart of a relation pointing either to a node or a way.\n",
    "- A **tag** is a (key, value) information attached to nodes, ways and relations to document in more detail the item.\n",
    "- **osm** is the root node in .osm files.\n",
    "- **note** and **meta** are metadata.\n",
    "\n",
    "We are now going to parse the XML file again to get the full path of each tag in the dataset. We need to use a SAX parser with a custom handler. Do not pay attention to callbacks, this will be explained in next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "from handler import OpenStreetMapXmlHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the handler in SAX parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = xml.sax.make_parser()\n",
    "with OpenStreetMapXmlHandler() as handler:\n",
    "    parser.setContentHandler(handler)\n",
    "    parser.parse(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'member': 17264,\n",
      " u'meta': 1,\n",
      " u'nd': 602009,\n",
      " u'node': 428787,\n",
      " u'note': 1,\n",
      " u'osm': 1,\n",
      " u'relation': 469,\n",
      " u'tag': 218929,\n",
      " u'way': 71960}\n"
     ]
    }
   ],
   "source": [
    "#Get tag counts\n",
    "pprint(handler.getTagsCount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned tag count is the same than the one we have calculated using `et.iterparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'member': set([u'osm.relation']),\n",
      " u'meta': set([u'osm']),\n",
      " u'nd': set([u'osm.way']),\n",
      " u'node': set([u'osm']),\n",
      " u'note': set([u'osm']),\n",
      " u'osm': set(['']),\n",
      " u'relation': set([u'osm']),\n",
      " u'tag': set([u'osm.node', u'osm.relation', u'osm.way']),\n",
      " u'way': set([u'osm'])}\n"
     ]
    }
   ],
   "source": [
    "#Get tag ancestors\n",
    "pprint(handler.getTagsAncestors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed later on:\n",
    "- **osm** element has no ancestor (it's root element)\n",
    "- **meta** and **note** only appear in **osm** element\n",
    "- **node**, **way** and **relation** are direct children of **osm**\n",
    "- **tag** can be used to document any of **node**, **way** and **relation**\n",
    "- **member** are only used in **relation** elements (to reference either nodes, ways or other relations)\n",
    "- **nd** are only used in **way** elements (to reference nodes)\n",
    "\n",
    "Such result will help us a lot when auditing [data quality](#Data quality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data quality'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality audit *[top](#Top)*\n",
    "\n",
    "This chapter is divided into 5 sections for each kind of data quality audit:\n",
    "- [Validity](#Data validity)\n",
    "- [Accuracy](#Data accuracy)\n",
    "- [Completeness](#Data completeness)\n",
    "- [Consistency](#Data consistency)\n",
    "- [Uniformity](#Data uniformity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data validity'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity *[audit](#Data quality)*\n",
    "\n",
    "Validity is about compliance to a schema. The data we have retrieved from OpenStreetMap servers is a XML file. It exists techniques to validate XML structures such as XML Schema. We won't use such technique here because schema is relatively simple and because XML files can be large enough so we want to stick to using SAX parser.\n",
    "\n",
    "Actually, the SAX content handler that has been introduced in previous [section](#XML data structure) will be helpful here as it's already able to list ancestors for each element. We can then define a schema in a similar form and compare both to see if there is any issue.\n",
    "\n",
    "The schema is a dictionnary structured this way:\n",
    "- key: element tag\n",
    "- value: dictionnary with the following keys / values:\n",
    "    - *ancestors*: List of any acceptable ancestor path. For example, the path ('osm.way') means that element shall be a children of a way element which itself is a children of a osm element.\n",
    "    - *minOccurences*: minimum number of element in the dataset (greater or equal to 0), optional\n",
    "    - *maxOccurences*: maximum number of element in the dataset (greater or equal to 1), optional\n",
    "    - *requiredAttributes*: list of attribute names that shall be defined for element\n",
    "    - *requiredChildren*: list of required children element\n",
    "    - *attributesFuncs*: list of callable objects to be run on the element attributes for further checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "#Function to check numbers\n",
    "check_digit = lambda name, attr: attr[name].isdigit()\n",
    "check_id_digit = functools.partial(check_digit, 'id')\n",
    "check_uid_digit = functools.partial(check_digit, 'uid')\n",
    "check_ref_digit = functools.partial(check_digit, 'ref')\n",
    "\n",
    "#Define a schema\n",
    "schema = {\n",
    "    #osm is root node. There shall be exactely one.\n",
    "    'osm': { \n",
    "        'ancestors': {''}, \n",
    "        'minOccurences': 1,\n",
    "        'maxOccurences': 1},\n",
    "    #meta shall be within osm element. There shall be exactely one of those.\n",
    "    'meta': {\n",
    "        'ancestors': {'osm'},\n",
    "        'minOccurences': 1,\n",
    "        'maxOccurences': 1},\n",
    "    #meta shall be within osm element. There shall be exactely one of those.\n",
    "    'note': {\n",
    "        'ancestors': {'osm'},\n",
    "        'minOccurences': 1,\n",
    "        'maxOccurences': 1},        \n",
    "    #node shall be within osm element. A node shall have id, lat (latitude) and lon (longitude) attributes.\n",
    "    #Additionally, lat shall be in the range [-90, 90] and longitude in the range [-180, 180]. Id shall be a digit \n",
    "    #number\n",
    "    'node': {\n",
    "        'ancestors': {'osm'},\n",
    "        'requiredAttributes': ['id', 'lat', 'lon', 'uid'],\n",
    "        'attributesFuncs': [lambda attr: -90 <= float(attr['lat']) <= 90, \n",
    "                            lambda attr: -180 <= float(attr['lon']) <= 180,\n",
    "                            check_id_digit,\n",
    "                            check_uid_digit]},\n",
    "    #way shall be within osm element. A way shall have id attribute. It shall have at least one nd children.\n",
    "    #id shall be a digit.\n",
    "    'way': {\n",
    "        'ancestors': {'osm'},\n",
    "        'requiredAttributes': ['id', 'uid'],\n",
    "        'requiredChildren': ['nd'],\n",
    "        'attributesFuncs': [check_id_digit, check_uid_digit]},\n",
    "    #nd shall be within way element. A nd shall have ref attribute. ref attribute shall be a digit.\n",
    "    'nd': {\n",
    "        'ancestors': {'osm.way'},\n",
    "        'requiredAttributes': ['ref'],\n",
    "        'attributesFuncs': [check_ref_digit]},\n",
    "    #relation shall be within a osm element. It shall have a id attribute and at least one member children. id shall\n",
    "    #be a digit\n",
    "    'relation': {\n",
    "        'ancestors': {'osm'},\n",
    "        'requiredAttributes': ['id', 'uid'],\n",
    "        'requiredChildren': ['member'],\n",
    "        'attributesFunc': [check_id_digit, check_uid_digit]},\n",
    "    #member shall be within a relation element. It shall have type, ref and role attributes. The type attribute shall\n",
    "    #be either way or node. The ref attribute shall be a digit.\n",
    "    'member': {\n",
    "        'ancestors': {'osm.relation'},\n",
    "        'requiredAttributes': ['type', 'ref', 'role'],\n",
    "        'attributesFuncs': [lambda attr: attr['type'] in ['way', 'node', 'relation'],\n",
    "                            check_ref_digit]},\n",
    "        \n",
    "    #tag shall be within node, way or relation. It shall have k and v attributes.\n",
    "    'tag': {\n",
    "        'ancestors': {'osm.node', 'osm.way', 'osm.relation'},\n",
    "        'requiredAttributes': ['k', 'v']},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have this schema validated, we are going to create a callback to be passed to SAX content handler we have created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from validity_audit import DataValidityAudit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function that will help us parsing and autiting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "\n",
    "#Define a method to parse and audit\n",
    "def parse_and_audit(dataset_path, audit=None):\n",
    "    \"\"\"\n",
    "    Parse XML dataset and perform audit quality.\n",
    "    \n",
    "    - dataset_path: path to the dataset to be parsed and audited\n",
    "    - audit: a sequence of audit objects\n",
    "    - return: sequence of nonconformities\n",
    "    \"\"\"\n",
    "    with OpenStreetMapXmlHandler() as handler:\n",
    "        if audit is not None:\n",
    "            for obj in audit:\n",
    "                handler.registerStartEventCallback(obj.startEventCallback)\n",
    "                handler.registerEndEventCallback(obj.endEventCallback)\n",
    "        parser = xml.sax.make_parser()\n",
    "        parser.setContentHandler(handler)\n",
    "        parser.parse(dataset_path)\n",
    "    nonconformities = []\n",
    "    if audit is not None:\n",
    "        for obj in audit:\n",
    "            nonconformities.extend(obj.getNonconformities())\n",
    "    return nonconformities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 52 ms, total: 15.3 s\n",
      "Wall time: 15.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parse and audit\n",
    "audit = [DataValidityAudit(schema)]\n",
    "%time nonconformities = parse_and_audit(dataset_path, audit)\n",
    "display(HTML(tabulate.tabulate(nonconformities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned list above shall be empty. It means that no nonconfirmity has been detected for validity audit. The data we get from OpenStreetMap may be trusted in terms of schema compliance.\n",
    "\n",
    "The `%timeit` Jupyter magic command enables to monitor how much time it takes to parse and audit the data. As a reference it takes approximately 15 seconds to parse and audit the dataset of around 100 Mb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data accuracy'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy *[audit](#Data quality)*\n",
    "\n",
    "Accuracy is a measurement of coformity with gold standard. On a dataset such as the one from OpenStreetMap it may be difficult to find a gold standard. We are then going to limit this audit to values that are sometimes provided in the dataset for items which represents a town:\n",
    "- INSEE indentifier (ref:INSEE in the above example)\n",
    "- Population\n",
    "- Date of last census (source:population in the above example)\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```xml\n",
    "<node id=\"26691412\" lat=\"43.5827846\" lon=\"1.3466543\" version=\"17\" timestamp=\"2017-08-22T17:20:54Z\" changeset=\"51349577\" uid=\"6523296\" user=\"ccampguilhem\">\n",
    "    <tag k=\"addr:postcode\" v=\"31170\"/>\n",
    "    <tag k=\"name\" v=\"Tournefeuille\"/>\n",
    "    <tag k=\"name:fr\" v=\"Tournefeuille\"/>\n",
    "    <tag k=\"name:oc\" v=\"TornafuÃÂ¨lha\"/>\n",
    "    <tag k=\"place\" v=\"town\"/>\n",
    "    <tag k=\"population\" v=\"26674\"/>\n",
    "    <tag k=\"ref:FR:SIREN\" v=\"213105570\"/>\n",
    "    <tag k=\"ref:INSEE\" v=\"31557\"/>\n",
    "    <tag k=\"source:population\" v=\"INSEE 2014\"/>\n",
    "    <tag k=\"wikidata\" v=\"Q328022\"/>\n",
    "    <tag k=\"wikipedia\" v=\"fr:Tournefeuille\"/>\n",
    "</node>\n",
    "```\n",
    "\n",
    "But this information may also be attached to a relation element instead:\n",
    "```xml\n",
    "<relation id=\"158881\" version=\"20\" timestamp=\"2017-06-22T16:33:19Z\" changeset=\"49751028\" uid=\"94578\" user=\"andygol\">\n",
    "    <member type=\"node\" ref=\"534672451\" role=\"admin_centre\"/>\n",
    "    <member type=\"way\" ref=\"36353842\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"166581580\" role=\"outer\"/>\n",
    "    ...\n",
    "    <member type=\"way\" ref=\"502733025\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"502733024\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"36353843\" role=\"outer\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"31820\"/>\n",
    "    <tag k=\"admin_level\" v=\"8\"/>\n",
    "    <tag k=\"boundary\" v=\"administrative\"/>\n",
    "    <tag k=\"name\" v=\"Pibrac\"/>\n",
    "    <tag k=\"name:fr\" v=\"Pibrac\"/>\n",
    "    <tag k=\"name:ru\" v=\"ÐÐ¸Ð±ÑÐ°Ðº\"/>\n",
    "    <tag k=\"name:uk\" v=\"ÐÑÐ±ÑÐ°Ðº\"/>\n",
    "    <tag k=\"name:zh\" v=\"ç®å¸æå\"/>\n",
    "    <tag k=\"population\" v=\"8091\"/>\n",
    "    <tag k=\"ref:FR:SIREN\" v=\"213104177\"/>\n",
    "    <tag k=\"ref:INSEE\" v=\"31417\"/>\n",
    "    <tag k=\"source:population\" v=\"INSEE 2013\"/>\n",
    "```\n",
    "\n",
    "Our audit code shall take this into account.\n",
    "\n",
    "For this example, I have updated the OpenStreetMap database manually to match official data published by [INSEE](https://www.insee.fr/en/accueil). I will use INSEE data as gold standard (see [here](https://www.insee.fr/fr/statistiques/1405599?geo=COM-31557+COM-31291+COM-31149+COM-31424+COM-31157+COM-31417)). The last census in my region is from 2014.\n",
    "\n",
    "We are going to define a gold standard in a dictionnary for few towns in the surrounding of Tournefeuille. If you have selected a user-defined area map, it may not be suitable to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Used to convert digit in XML with thoudand separators into a Python integer\n",
    "convert_to_int = lambda x: int(x.replace(\" \", \"\"))\n",
    "\n",
    "gold_standard_insee = {\n",
    "    u'Tournefeuille': {\n",
    "        'population': (convert_to_int, 26674),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31557)},\n",
    "    u'LÃ©guevin': {\n",
    "        'population': (convert_to_int, 8892),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31291)},\n",
    "    u'Colomiers': {\n",
    "        'population': (convert_to_int, 38541),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31149)},\n",
    "    u'Plaisance-du-Touch': {\n",
    "        'population': (convert_to_int, 17278),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31424)},\n",
    "    u'Cugnaux': {\n",
    "        'population': (convert_to_int, 17004),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31157)},\n",
    "    u'Pibrac': {\n",
    "        'population': (convert_to_int, 8226),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31417)},\n",
    "    u'Toulouse': {\n",
    "        'population': (convert_to_int, 466297),\n",
    "        'source:population': (str, 'INSEE 2014'),\n",
    "        'ref:INSEE': (convert_to_int, 31555)},       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an audit class for accuracy. It will compare each information from items having a \"population\" tag to the standard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from accuracy_audit import DataAccuracyAudit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.92 s, sys: 0 ns, total: 6.92 s\n",
      "Wall time: 6.92 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>Accuracy</td><td>\"INSEE 2013\" value provided for \"source:population\" of Plaisance-du-Touch is inaccurate. Expected value is \"INSEE 2014\".</td></tr>\n",
       "<tr><td>Accuracy</td><td>\"16091\" value provided for \"population\" of Plaisance-du-Touch is inaccurate. Expected value is \"17278\".                 </td></tr>\n",
       "<tr><td>Accuracy</td><td>\"INSEE 2013\" value provided for \"source:population\" of Colomiers is inaccurate. Expected value is \"INSEE 2014\".         </td></tr>\n",
       "<tr><td>Accuracy</td><td>\"35186\" value provided for \"population\" of Colomiers is inaccurate. Expected value is \"38541\".                          </td></tr>\n",
       "<tr><td>Accuracy</td><td>\"INSEE 2013\" value provided for \"source:population\" of Toulouse is inaccurate. Expected value is \"INSEE 2014\".          </td></tr>\n",
       "<tr><td>Accuracy</td><td>\"441802\" value provided for \"population\" of Toulouse is inaccurate. Expected value is \"466297\".                         </td></tr>\n",
       "<tr><td>Accuracy</td><td>\"INSEE 2013\" value provided for \"source:population\" of Pibrac is inaccurate. Expected value is \"INSEE 2014\".            </td></tr>\n",
       "<tr><td>Accuracy</td><td>\"8091\" value provided for \"population\" of Pibrac is inaccurate. Expected value is \"8226\".                               </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parse and audit\n",
    "audit = [DataAccuracyAudit(gold_standard_insee)]\n",
    "%time nonconformities = parse_and_audit(dataset_path, audit)\n",
    "display(HTML(tabulate.tabulate(nonconformities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some accuracy issues are reported because data in OpenStreetMap is not up to date with census of 2014.\n",
    "No issue is reported for Tournefeuille because I have manually updated the OpenStreetMap database.\n",
    "\n",
    "There are many more accuracy checks that we can do. For example, building with commercial activities have their phone number and web site mentioned in the OpenStreetMap database. Accuracy would have been assessed by checking existence of web site or by comparing phone number to official records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data completeness'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness *[audit](#Data quality)*\n",
    "\n",
    "Assessing completeness of data is a difficult task. We'll do the work for pharmacies. We will use another standard: [Pages Jaunes](https://www.pagesjaunes.fr/annuaire/tournefeuille-31/pharmacies). Pages Jaunes provides the same kind of services than Yellow Pages.\n",
    "\n",
    "Here is the list of pharmacies we expect to find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_standard_pages_jaunes = [\n",
    "    (u'Pharmacie Denise RibÃ¨re', (u'2', u'Rue Platanes', 31170, u'Tournefeuille')),\n",
    "    (u'Pharmacie De La RamÃ©e', (u'102', u'Chemin Larramet', 31170, u'Tournfeuille')),\n",
    "    (u'Pharmacie Cap 2000', (u'1', u'Boulevard Jean Gay', 31170, u'Tournfeuille')),\n",
    "    (u'Pharmacie De La Commanderie', (u'110', u'Avenue Marquisat', 31170, u'Tournfeuille')),\n",
    "    (u'Pharmacie Julien RiviÃ©re-Sacaze', (u'18', u'Boulevard EugÃ¨ne Montel', 31170, u'Tournfeuille')),\n",
    "    (u'Pharmacie Arc En Ciel', (u'19', u'Avenue Alphonse Daudet', 31170, u'Tournfeuille')),\n",
    "    (u'Pharmacie Du Centre', (u'67', u'Rue Gaston Doumergue', 31170, u'Tournfeuille')),\n",
    "    (u'La Pharmacie Du Vieux Pigeonnier', (u'3', u'Rue Hector Berlioz', 31170, u'Tournfeuille')),\n",
    "    (u'Pharmacie De Pahin', (u'37', u'Chemin Fournaulis', 31170, u'Tournfeuille'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an audit class for completeness. It will compare each information from items having a \"amenity/pharmacy\" tag to the standard above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from completeness_audit import DataCompletenessAudit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.74 s, sys: 12 ms, total: 6.75 s\n",
      "Wall time: 6.75 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie Saint-Simon\" found but not expected.          </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie de Lardenne\" found but not expected.          </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie du Touch\" found but not expected.             </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie Bruna Rosso\" found but not expected.          </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie des Marots\" found but not expected.           </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie Ãdourad Cabane\" found but not expected.       </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie de la Paderne\" found but not expected.        </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie des Jasmins\" found but not expected.          </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie des Pradettes\" found but not expected.        </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmaccie Arc-en-Ciel\" found but not expected.         </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie Robin\" found but not expected.                </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie RibÃ¨re\" found but not expected.               </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie des Tibaous\" found but not expected.          </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie Hebraud Meneghetti\" found but not expected.   </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie Sol\" found but not expected.                  </td></tr>\n",
       "<tr><td>Warning     </td><td>Pharmacy \"Pharmacie du Ramelet Moundi\" found but not expected.    </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"Pharmacie Denise RibÃ¨re\" is missing in dataset.         </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"Pharmacie Julien RiviÃ©re-Sacaze\" is missing in dataset. </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"Pharmacie Arc En Ciel\" is missing in dataset.           </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"La Pharmacie Du Vieux Pigeonnier\" is missing in dataset.</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parse and audit\n",
    "audit = [DataCompletenessAudit(gold_standard_pages_jaunes, warnings=True)]\n",
    "%time nonconformities = parse_and_audit(dataset_path, audit)\n",
    "display(HTML(tabulate.tabulate(nonconformities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 pharmacies are reported as missing. Some others are reported as present but not expected. This is because dataset extends over boundary of Tournefeuille. We can notice two things:\n",
    "\n",
    "- Pharmacie Denise RibÃ¨re is missing and Pharmacie RibÃ¨re has been found. It may be the pharmacy we expected.\n",
    "- Pharmacie Arc En Ciel is missing and Pharmac**c**ie Arc-en-Ciel has been found. Our string comparison function convert to lower case and replace - by space, but there is a typo in OpenStreetMap database. Use of [fuzzy string](https://streamhacker.com/2011/10/31/fuzzy-string-matching-python/) matching algorithms might have helped in this situation.\n",
    "\n",
    "For the last two missing items (La Pharmacie Du Vieux Pigeonnier and Pharmacie Julien RiviÃ©re-Sacaze), it seems at first glance that dataset is simply uncomplete. But after a closer look to Pages Jaunes on the location of pharmacies, it seems that they match position of Pharmacie Hebraud Meneghetti and Pharmacie Robin.\n",
    "\n",
    "A simple rename of pharmacies in OpenStreetMap dataset would be enought to ensure a 100% completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data consistency'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency *[audit](#Data quality)*\n",
    "\n",
    "Consistency audit consits in finding contradictory information in the dataset or find issues that prevent us from using some information in the dataset.\n",
    "\n",
    "In a previous [section](#XML data structure), we have seen that **relation** elements refer to **node**, **way** or other **relation** through the **member** element. Similarly, **way** elements refer to nodes throught **nd** item.\n",
    "\n",
    "A consistent dataset would provide **relation** and **way** pointing to **node** and **way** also present in the dataset. This is the check we are going to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from consistency_audit import DataConsistencyAudit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.6 s, sys: 24 ms, total: 9.62 s\n",
      "Wall time: 9.61 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>Consistency</td><td>635 ways refer to non-present entities out of 71960 (0.9%)    </td></tr>\n",
       "<tr><td>Consistency</td><td>145 relations refer to non-present entities out of 469 (30.9%)</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parse and audit\n",
    "audit = [DataConsistencyAudit()]\n",
    "%time nonconformities = parse_and_audit(dataset_path, audit)\n",
    "display(HTML(tabulate.tabulate(nonconformities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ways (a very low percentage) refer to non-present nodes. A significant number of relations refer to non-present entities. One possible explanation is that towns may be not completly extracted and relation defines town boundaries with nodes or ways. Those nodes or ways are missing because they are out of the box we have extracted from OpenStreetMap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data uniformity'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniformity *[audit](#Data quality)*\n",
    "\n",
    "To audit uniformity, we are going to focus on the way addresses are provided in the dataset.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "```xml\n",
    "<relation id=\"1246249\" version=\"2\" timestamp=\"2017-08-21T10:30:22Z\" changeset=\"51299391\" uid=\"922338\" user=\"HervÃ© TUC\">\n",
    "    <member type=\"way\" ref=\"74688949\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"74695300\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"74692941\" role=\"outer\"/>\n",
    "    <member type=\"way\" ref=\"74688530\" role=\"outer\"/>\n",
    "    <tag k=\"addr:city\" v=\"Toulouse\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"42\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"31057\"/>\n",
    "    <tag k=\"addr:street\" v=\"Avenue Gaspard Coriolis\"/>\n",
    "    ...\n",
    "```\n",
    "\n",
    "The item (a relation here) is documented with tags addr:city, addr:housenumber, addr:postcode, addr:street. The addresses in the dataset will be considered uniform if each of them contain all those components. In addition, the way addr:street are recorded will be analyzed to check if mulitple ways of writing [Rue, Avenue, Boulevard, Place, ...] are used. The audit class will report any non-uniformity throughout the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from uniformity_audit import DataUniformityAudit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.71 s, sys: 32 ms, total: 7.74 s\n",
      "Wall time: 7.73 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>Uniformity</td><td>2318 elements miss the following fields: city, postcode.           </td></tr>\n",
       "<tr><td>Uniformity</td><td>2210 elements miss the following fields: postcode.                 </td></tr>\n",
       "<tr><td>Uniformity</td><td>59 elements miss the following fields: city.                       </td></tr>\n",
       "<tr><td>Uniformity</td><td>8 elements miss the following fields: housenumber.                 </td></tr>\n",
       "<tr><td>Uniformity</td><td>12 elements miss the following fields: city, postcode, housenumber.</td></tr>\n",
       "<tr><td>Uniformity</td><td>13 elements miss the following fields: postcode, housenumber.      </td></tr>\n",
       "<tr><td>Uniformity</td><td>11 elements miss the following fields: city, housenumber.          </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'Boulevard', u'Rond-Point', u'rue', u'impasse', u'avenue', u'Route', u'Passage', u'Impasse', u'Av.', u'Chemin', u'place', u'Clos', u'Place', u'all\\xe9e', u'Rue', u'Avenue', u'All\\xe9e'])\n"
     ]
    }
   ],
   "source": [
    "#Parse and audit\n",
    "audit = [DataUniformityAudit(warnings=False)]\n",
    "%time nonconformities = parse_and_audit(dataset_path, audit)\n",
    "display(HTML(tabulate.tabulate(nonconformities, tablefmt='html')))\n",
    "streets_patterns = audit[0].getStreetsPatterns()\n",
    "print streets_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of uniformity of providing the same address components, the most common issue is to not have postcode and city. Few times housenumber is also missing. Fixing housenumber automatically seems difficult. Fixing postcode may be easy in the case city, as recorded in OpenStreetMap has a single postcode and item has a city attached. There is nothing obvious we can do for items having no postcode and city fields. One possible solution would be to check inclusion of node (given its latitude / longitude) in the polygon delimiting city (as provided by **relation** elements) which can be solved with a little [maths](http://geomalgorithms.com/a03-_inclusion.html).\n",
    "\n",
    "There is also a lack of uniformity in the way streets are recorded. For example we can see Av. avenue, or Avenue but this is not a big deal and be fixed easilly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Audit conclusion'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion *[audit](#Data quality)*\n",
    "\n",
    "The audit performed is rather incomplete in terms of check that can be performed. But we have seen how we can audit any kind of nonconformity (validity, accuracy, completeness, consistency and uniformity).\n",
    "\n",
    "Yet, the frontier between each type of audit may be tenuous:\n",
    "- the completeness issues indentified with missing pharmacies turned out to be an accuracy problem (pharmacies are in the dataset but with a different name).\n",
    "- the inconsistency issue with nodes or ways referenced either in ways or relations but missing from dataset may be seen as a completeness issue. Nodes and ways probably exist in the full OpenStreetMap database, so this is probably more related to how the data from full database is extracted with a box selection.\n",
    "- the uniformity issues in the way addresses are recorded may also be seen as a completeness issue because housenumbers are missing.\n",
    "\n",
    "The classification of nonconformities is not that important, but the list (validity, accuracy, completeness, consistency and uniformity) is probably a good hint to be sure we do not forget some kind of checks.\n",
    "\n",
    "On large datasets, it seems not impossible but very tedious to run a full quality audit. Knowing the scope of analysis helps in selecting the minimum set of audits to run on the dataset.\n",
    "\n",
    "Using an iterative parser is clearly an additional difficulty in writing audit code, things would have been much simpler by using a full-parser like the ones provided by [lxml](http://lxml.de/) library. lxml also implements pretty advanced XPath requests that would have made both auditing and cleaning much faster.\n",
    "\n",
    "The following code wraps all audit tasks and returns a table with all kind of nonconformities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.5 s, sys: 44 ms, total: 22.6 s\n",
      "Wall time: 22.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>Accuracy    </td><td>\"INSEE 2013\" value provided for \"source:population\" of Plaisance-du-Touch is inaccurate. Expected value is \"INSEE 2014\".</td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"16091\" value provided for \"population\" of Plaisance-du-Touch is inaccurate. Expected value is \"17278\".                 </td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"INSEE 2013\" value provided for \"source:population\" of Colomiers is inaccurate. Expected value is \"INSEE 2014\".         </td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"35186\" value provided for \"population\" of Colomiers is inaccurate. Expected value is \"38541\".                          </td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"INSEE 2013\" value provided for \"source:population\" of Toulouse is inaccurate. Expected value is \"INSEE 2014\".          </td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"441802\" value provided for \"population\" of Toulouse is inaccurate. Expected value is \"466297\".                         </td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"INSEE 2013\" value provided for \"source:population\" of Pibrac is inaccurate. Expected value is \"INSEE 2014\".            </td></tr>\n",
       "<tr><td>Accuracy    </td><td>\"8091\" value provided for \"population\" of Pibrac is inaccurate. Expected value is \"8226\".                               </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"Pharmacie Denise RibÃ¨re\" is missing in dataset.                                                               </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"Pharmacie Julien RiviÃ©re-Sacaze\" is missing in dataset.                                                       </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"Pharmacie Arc En Ciel\" is missing in dataset.                                                                 </td></tr>\n",
       "<tr><td>Completeness</td><td>Pharmacy \"La Pharmacie Du Vieux Pigeonnier\" is missing in dataset.                                                      </td></tr>\n",
       "<tr><td>Consistency </td><td>635 ways refer to non-present entities out of 71960 (0.9%)                                                              </td></tr>\n",
       "<tr><td>Consistency </td><td>145 relations refer to non-present entities out of 469 (30.9%)                                                          </td></tr>\n",
       "<tr><td>Uniformity  </td><td>2318 elements miss the following fields: city, postcode.                                                                </td></tr>\n",
       "<tr><td>Uniformity  </td><td>2210 elements miss the following fields: postcode.                                                                      </td></tr>\n",
       "<tr><td>Uniformity  </td><td>59 elements miss the following fields: city.                                                                            </td></tr>\n",
       "<tr><td>Uniformity  </td><td>8 elements miss the following fields: housenumber.                                                                      </td></tr>\n",
       "<tr><td>Uniformity  </td><td>12 elements miss the following fields: city, postcode, housenumber.                                                     </td></tr>\n",
       "<tr><td>Uniformity  </td><td>13 elements miss the following fields: postcode, housenumber.                                                           </td></tr>\n",
       "<tr><td>Uniformity  </td><td>11 elements miss the following fields: city, housenumber.                                                               </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parse and audit\n",
    "full_audit = [DataValidityAudit(schema), DataAccuracyAudit(gold_standard_insee), \n",
    "         DataCompletenessAudit(gold_standard_pages_jaunes), DataConsistencyAudit(), DataUniformityAudit()]\n",
    "%time nonconformities = parse_and_audit(dataset_path, full_audit)\n",
    "display(HTML(tabulate.tabulate(nonconformities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have not found any validity issue. The XML structure is pretty simple and OpenStreetMap provides XML files that can be trusted in terms of structure. The issues come from data that has been provided by users.\n",
    "\n",
    "In the [next section](#Data cleaning), we are going to clean this dataset before importing it into a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data cleaning'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning *[top](#Top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Method'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method *[cleaning](#Data cleaning)*\n",
    "\n",
    "We are not going to clean the XML file we have downloaded from OpenStreetMap. As we need to parse it iteratively, writing a cleaning algorithm would be pretty difficult.\n",
    "\n",
    "Here are the steps we are going to follow:\n",
    "\n",
    "- Export dataset into a Python dictionnary-like structure\n",
    "- Clean the dictionnary\n",
    "- Save dictionnary into a JSON file\n",
    "\n",
    "**Note:** exporting the dataset to a dictionnary may be memory consumming. I have not used any technique here to reduce memory footprint but [shelve](https://docs.python.org/2/library/shelve.html?highlight=shelve#module-shelve) in standard library may help. Alternatives may be [diskcache](http://www.grantjenks.com/docs/diskcache/tutorial.html) and even directly [pymongo](https://api.mongodb.com/python/current/tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Converting to dictionnary-like structure'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to dictionnary-like structure *[cleaning](#Data cleaning)*\n",
    "\n",
    "We are going to use a similar technique than the one we had for data quality audit. We are basically going to plug a callback class to the SAX content handler to load OpenStreetMap dataset into a dictonnary-like structure.\n",
    "\n",
    "First we need to define the dictionnary structure we want to have:\n",
    "\n",
    "```python\n",
    "schema_dict = {\n",
    "    'nodes': [\n",
    "        {'osmid': 0, #the OpenStreetMap id of node\n",
    "         'latitude': 0., #latitude [-90, 90]\n",
    "         'longitude': 0., #longitude [-180, 180]\n",
    "         'userid': 0, #OpenStreetMap id of owner\n",
    "         'tags': [ #list of associated tags\n",
    "             {'key': '', #tag key\n",
    "              'value': ''}] #tag value\n",
    "        }\n",
    "    ]\n",
    "    'ways': [\n",
    "        {'osmid': 0, #the OpenStreetMap id of node\n",
    "         'userid': 0, #OpenStreetMap id of owner\n",
    "         'tags': [ #list of associated tags\n",
    "             {'key': '', #tag key\n",
    "              'value': ''}] #tag value\n",
    "         'nodes': [ ] #a list of nodes osmid\n",
    "        }\n",
    "    ]\n",
    "    'relations': [\n",
    "        {'osmid': 0, #the OpenStreetMap id of node\n",
    "         'userid': 0, #OpenStreetMap id of owner\n",
    "         'tags': [ #list of associated tags\n",
    "             {'key': '', #tag key\n",
    "              'value': ''}] #tag value\n",
    "         'nodes': [ ] #a list of nodes osmid\n",
    "         'ways': [ ] #a list of ways osmid\n",
    "         'relations': [ ] #a list of relations osmid\n",
    "        }\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dictionnary_export import DictionnaryExport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 68 ms, total: 10.3 s\n",
      "Wall time: 10.4 s\n",
      "428787 nodes exported.\n",
      "71960 ways exported.\n",
      "469 relations exported.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#Parse and extract\n",
    "parser = xml.sax.make_parser()\n",
    "dataset_dict = { }\n",
    "extract_class = DictionnaryExport(dataset_dict)\n",
    "with OpenStreetMapXmlHandler() as handler:\n",
    "    handler.registerStartEventCallback(extract_class.startEventCallback)\n",
    "    handler.registerEndEventCallback(extract_class.endEventCallback)\n",
    "    parser.setContentHandler(handler)\n",
    "    %time parser.parse(dataset_path)\n",
    "for kind in ['nodes', 'ways', 'relations']:\n",
    "    print \"{} {} exported.\".format(len(dataset_dict[kind]), kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All nodes, ways and relations have been exported. We can investigate few items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 43.5874494, 'userid': 568960, 'osmid': 5053431726, 'longitude': 1.349595, 'tags': [{'value': u'bicycle_parking', 'key': u'amenity'}]}\n"
     ]
    }
   ],
   "source": [
    "print dataset_dict['nodes'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nodes': [5052004973, 5052004972, 5052004971, 5052004970, 5052004973], 'userid': 568960, 'osmid': 517760389, 'tags': [{'value': u'parking', 'key': u'amenity'}]}\n"
     ]
    }
   ],
   "source": [
    "print dataset_dict['ways'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'osmid': 7494469, 'tags': [{'value': u'cemetery', 'key': u'landuse'}, {'value': u'Cimeti\\xe8re de Lardenne', 'key': u'name'}, {'value': u'multipolygon', 'key': u'type'}], 'ways': [24788185, 167601981], 'userid': 922338, 'relations': [], 'nodes': []}\n"
     ]
    }
   ],
   "source": [
    "print dataset_dict['relations'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the dataset in this format is much more handy. But making queries into it requires to write dedicated functions. That's all the point to use a database: anything to perform requests is already implemented for us. For now, it is convenient enough to perform the data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning accuracy issues'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning accuracy issues *[cleaning](#Data cleaning)*\n",
    "\n",
    "In data accuracy audit [section](#Data accuracy) we have spotted few accuracy issues regarging city populations. The idea here is just to update nodes and relations having population tag so thet match the INSEE standard.\n",
    "\n",
    "A simple function may be written for that purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean accuracy issues\n",
    "def clean_accuracy(dataset, standard):\n",
    "    \"\"\"\n",
    "    Clean accuracy issues according to INSEE standard.\n",
    "    \n",
    "    The dataset is modified in-place.\n",
    "    \n",
    "    - dataset: dataset to be cleaned\n",
    "    - standard: gold standard for accuracy criteria (INSEE gold standard)\n",
    "    - return: list of updated nodes index and list of updated relations index\n",
    "    \"\"\"\n",
    "    nodes = [ ]\n",
    "    relations = [ ]\n",
    "    def process_item(kind, modified):\n",
    "        #Process nodes\n",
    "        for iitem, item in enumerate(dataset[kind]):\n",
    "            tag_keys = [tag['key'] for tag in item['tags']]\n",
    "            #Has population tag?\n",
    "            try:\n",
    "                population_index = tag_keys.index('population')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            #Update\n",
    "            try:\n",
    "                city = item['tags'][tag_keys.index('name:fr')]['value']\n",
    "            except ValueError:\n",
    "                city = item['tags'][tag_keys.index('name')]['value']\n",
    "            source_population_index = tag_keys.index('source:population')\n",
    "            if item['tags'][source_population_index]['value'] != standard[city]['source:population'][1]:\n",
    "                item['tags'][source_population_index]['value'] = standard[city]['source:population'][1]\n",
    "                item['tags'][population_index]['value'] = standard[city]['population'][1]\n",
    "                print \"updated {} {} ({}).\".format(kind.rstrip('s'), item[\"osmid\"], city)\n",
    "                modified.append(iitem)\n",
    "            else:\n",
    "                print \"{} {} ({}) is left unchanged.\".format(kind.rstrip('s'), item[\"osmid\"], city)\n",
    "    #Process nodes\n",
    "    process_item(\"nodes\", nodes)\n",
    "    #Process relations\n",
    "    process_item(\"relations\", relations)    \n",
    "    return nodes, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 26691412 (Tournefeuille) is left unchanged.\n",
      "updated node 26691742 (Plaisance-du-Touch).\n",
      "updated relation 35078 (Colomiers).\n",
      "updated relation 35738 (Toulouse).\n",
      "updated relation 158881 (Pibrac).\n",
      "CPU times: user 404 ms, sys: 44 ms, total: 448 ms\n",
      "Wall time: 374 ms\n",
      "{'latitude': 43.5653012,\n",
      " 'longitude': 1.2978343,\n",
      " 'osmid': 26691742,\n",
      " 'tags': [{'key': u'addr:postcode', 'value': u'31830'},\n",
      "          {'key': u'name', 'value': u'Plaisance-du-Touch'},\n",
      "          {'key': u'name:fr', 'value': u'Plaisance-du-Touch'},\n",
      "          {'key': u'name:oc', 'value': u'Plasen\\xe7a-deu-Toish'},\n",
      "          {'key': u'place', 'value': u'town'},\n",
      "          {'key': u'population', 'value': 17278},\n",
      "          {'key': u'ref:FR:SIREN', 'value': u'213104243'},\n",
      "          {'key': u'ref:INSEE', 'value': u'31424'},\n",
      "          {'key': u'source:population', 'value': 'INSEE 2014'},\n",
      "          {'key': u'wikidata', 'value': u'Q608231'}],\n",
      " 'userid': 3057995}\n",
      "{'nodes': [26692194],\n",
      " 'osmid': 35078,\n",
      " 'relations': [],\n",
      " 'tags': [{'key': u'addr:postcode', 'value': u'31770'},\n",
      "          {'key': u'admin_level', 'value': u'8'},\n",
      "          {'key': u'boundary', 'value': u'administrative'},\n",
      "          {'key': u'name', 'value': u'Colomiers'},\n",
      "          {'key': u'name:ca', 'value': u'Colomi\\xe8rs'},\n",
      "          {'key': u'name:fa',\n",
      "           'value': u'\\u06a9\\u0648\\u0644\\u0648\\u0645\\u06cc\\u0647'},\n",
      "          {'key': u'name:fr', 'value': u'Colomiers'},\n",
      "          {'key': u'name:ja', 'value': u'\\u30b3\\u30ed\\u30df\\u30a8'},\n",
      "          {'key': u'name:mzn',\n",
      "           'value': u'\\u06a9\\u0648\\u0644\\u0648\\u0645\\u06cc\\u0647'},\n",
      "          {'key': u'name:oc', 'value': u'Colom\\xe8rs'},\n",
      "          {'key': u'name:ru',\n",
      "           'value': u'\\u041a\\u043e\\u043b\\u043e\\u043c\\u044c\\u0435'},\n",
      "          {'key': u'name:sr',\n",
      "           'value': u'\\u041a\\u043e\\u043b\\u043e\\u043c\\u0458\\u0435'},\n",
      "          {'key': u'name:uk',\n",
      "           'value': u'\\u041a\\u043e\\u043b\\u043e\\u043c\\u2019\\u0454'},\n",
      "          {'key': u'name:zh', 'value': u'\\u79d1\\u6d1b\\u7c73\\u8036\\u5c14'},\n",
      "          {'key': u'population', 'value': 38541},\n",
      "          {'key': u'ref:FR:SIREN', 'value': u'213101496'},\n",
      "          {'key': u'ref:INSEE', 'value': u'31149'},\n",
      "          {'key': u'source:addr:postcode',\n",
      "           'value': u'source of postcode is from osm nodes'},\n",
      "          {'key': u'source:population', 'value': 'INSEE 2014'},\n",
      "          {'key': u'type', 'value': u'boundary'},\n",
      "          {'key': u'website', 'value': u'http://www.ville-colomiers.fr/'},\n",
      "          {'key': u'wikidata', 'value': u'Q318071'},\n",
      "          {'key': u'wikipedia', 'value': u'fr:Colomiers'}],\n",
      " 'userid': 25762,\n",
      " 'ways': [36353844,\n",
      "          31433138,\n",
      "          27509160,\n",
      "          27509159,\n",
      "          125089836,\n",
      "          123232545,\n",
      "          123412766,\n",
      "          123412765,\n",
      "          123412767,\n",
      "          123412777,\n",
      "          236900773,\n",
      "          225823029,\n",
      "          225823028,\n",
      "          125089835,\n",
      "          486218377,\n",
      "          27447131,\n",
      "          27447130]}\n",
      "{'nodes': [26686518],\n",
      " 'osmid': 35738,\n",
      " 'relations': [4243680, 4246121, 4248911, 4249032, 4499125, 4499126, 4499124],\n",
      " 'tags': [{'key': u'addr:postcode',\n",
      "           'value': u'31000;31100;31200;31300;31400;31500'},\n",
      "          {'key': u'admin_level', 'value': u'8'},\n",
      "          {'key': u'boundary', 'value': u'administrative'},\n",
      "          {'key': u'name', 'value': u'Toulouse / Tolosa'},\n",
      "          {'key': u'name:am', 'value': u'\\u1271\\u1209\\u12dd'},\n",
      "          {'key': u'name:ar', 'value': u'\\u062a\\u0648\\u0644\\u0648\\u0632'},\n",
      "          {'key': u'name:az', 'value': u'Tuluza'},\n",
      "          {'key': u'name:be',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u044e\\u0437\\u0430'},\n",
      "          {'key': u'name:bg',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:bo',\n",
      "           'value': u'\\u0f4a\\u0f7c\\u0f60\\u0f74\\u0f0b\\u0f63\\u0f7c\\u0f60\\u0f74\\u0f0b\\u0f66\\u0f7a\\u0f0d'},\n",
      "          {'key': u'name:br', 'value': u'Toloza'},\n",
      "          {'key': u'name:ca', 'value': u'Tolosa de Llenguadoc'},\n",
      "          {'key': u'name:ce',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:co', 'value': u'Tolosa'},\n",
      "          {'key': u'name:csb', 'value': u'T\\xf3l\\xf3za'},\n",
      "          {'key': u'name:de', 'value': u'Toulouse'},\n",
      "          {'key': u'name:el',\n",
      "           'value': u'\\u03a4\\u03bf\\u03c5\\u03bb\\u03bf\\u03cd\\u03b6'},\n",
      "          {'key': u'name:eo', 'value': u'Tuluzo'},\n",
      "          {'key': u'name:es', 'value': u'Tolosa'},\n",
      "          {'key': u'name:eu', 'value': u'Tolosa Okzitania'},\n",
      "          {'key': u'name:fa', 'value': u'\\u062a\\u0648\\u0644\\u0648\\u0632'},\n",
      "          {'key': u'name:fr', 'value': u'Toulouse'},\n",
      "          {'key': u'name:frp', 'value': u'Tolosa'},\n",
      "          {'key': u'name:gl', 'value': u'Tolosa'},\n",
      "          {'key': u'name:he', 'value': u'\\u05d8\\u05d5\\u05dc\\u05d5\\u05d6'},\n",
      "          {'key': u'name:hi',\n",
      "           'value': u'\\u0924\\u0941\\u0932\\u0942\\u091c\\u093c'},\n",
      "          {'key': u'name:hy',\n",
      "           'value': u'\\u0539\\u0578\\u0582\\u056c\\u0578\\u0582\\u0566'},\n",
      "          {'key': u'name:it', 'value': u'Tolosa'},\n",
      "          {'key': u'name:ja',\n",
      "           'value': u'\\u30c8\\u30a5\\u30fc\\u30eb\\u30fc\\u30ba'},\n",
      "          {'key': u'name:ka',\n",
      "           'value': u'\\u10e2\\u10e3\\u10da\\u10e3\\u10d6\\u10d0'},\n",
      "          {'key': u'name:kk',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:kn',\n",
      "           'value': u'\\u0ca4\\u0cc2\\u0cb2\\u0cc2\\u0cb8\\u0ccd'},\n",
      "          {'key': u'name:ko', 'value': u'\\ud234\\ub8e8\\uc988'},\n",
      "          {'key': u'name:la', 'value': u'Tolosa'},\n",
      "          {'key': u'name:lt', 'value': u'Tul\\u016bza'},\n",
      "          {'key': u'name:lv', 'value': u'Tul\\u016bza'},\n",
      "          {'key': u'name:mhr',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u043e'},\n",
      "          {'key': u'name:ml',\n",
      "           'value': u'\\u0d1f\\u0d42\\u0d33\\u0d42\\u0d38\\u0d4d'},\n",
      "          {'key': u'name:mr', 'value': u'\\u0924\\u0941\\u0932\\u0942\\u091d'},\n",
      "          {'key': u'name:mzn', 'value': u'\\u062a\\u0648\\u0644\\u0648\\u0632'},\n",
      "          {'key': u'name:nap', 'value': u'Tolosa'},\n",
      "          {'key': u'name:oc', 'value': u'Tolosa'},\n",
      "          {'key': u'name:os', 'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\xe6'},\n",
      "          {'key': u'name:pa',\n",
      "           'value': u'\\u0a1f\\u0a41\\u0a32\\u0a42\\u0a1c\\u0a3c'},\n",
      "          {'key': u'name:pl', 'value': u'Tuluza'},\n",
      "          {'key': u'name:pms', 'value': u'Tolosa'},\n",
      "          {'key': u'name:pnb', 'value': u'\\u062a\\u0648\\u0644\\u0648\\u0632'},\n",
      "          {'key': u'name:ru',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:sc', 'value': u'Tolosa'},\n",
      "          {'key': u'name:scn', 'value': u'Tolosa'},\n",
      "          {'key': u'name:sr',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:ta',\n",
      "           'value': u'\\u0ba4\\u0bc1\\u0bb2\\u0bc2\\u0bb8\\u0bcd'},\n",
      "          {'key': u'name:th', 'value': u'\\u0e15\\u0e39\\u0e25\\u0e39\\u0e0b'},\n",
      "          {'key': u'name:tt',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:uk',\n",
      "           'value': u'\\u0422\\u0443\\u043b\\u0443\\u0437\\u0430'},\n",
      "          {'key': u'name:ur', 'value': u'\\u062a\\u0648\\u0644\\u0648\\u0632'},\n",
      "          {'key': u'name:vec', 'value': u'To\\u0142oxa'},\n",
      "          {'key': u'name:xmf',\n",
      "           'value': u'\\u10e2\\u10e3\\u10da\\u10e3\\u10d6\\u10d0'},\n",
      "          {'key': u'name:yi', 'value': u'\\u05d8\\u05d5\\u05dc\\u05d5\\u05d6'},\n",
      "          {'key': u'name:yue', 'value': u'\\u5716\\u9b6f\\u8332'},\n",
      "          {'key': u'name:zh', 'value': u'\\u56fe\\u5362\\u5179'},\n",
      "          {'key': u'population', 'value': 466297},\n",
      "          {'key': u'ref:FR:SIREN', 'value': u'213105554'},\n",
      "          {'key': u'ref:INSEE', 'value': u'31555'},\n",
      "          {'key': u'short_name:ca', 'value': u'Tolosa'},\n",
      "          {'key': u'short_name:eu', 'value': u'Tolosa'},\n",
      "          {'key': u'source', 'value': u'ieo-bdtopoc'},\n",
      "          {'key': u'source:population', 'value': 'INSEE 2014'},\n",
      "          {'key': u'type', 'value': u'boundary'},\n",
      "          {'key': u'website', 'value': u'http://www.toulouse.fr/'},\n",
      "          {'key': u'wikidata', 'value': u'Q7880'},\n",
      "          {'key': u'wikipedia', 'value': u'fr:Toulouse'}],\n",
      " 'userid': 6467578,\n",
      " 'ways': [122932233,\n",
      "          31277524,\n",
      "          175498552,\n",
      "          322997447,\n",
      "          322997450,\n",
      "          175498556,\n",
      "          27509244,\n",
      "          125089836,\n",
      "          123232545,\n",
      "          123412766,\n",
      "          123412765,\n",
      "          123412767,\n",
      "          123412777,\n",
      "          236900773,\n",
      "          225823029,\n",
      "          225823028,\n",
      "          125089835,\n",
      "          486218377,\n",
      "          27447131,\n",
      "          122952969,\n",
      "          74874684,\n",
      "          17161492,\n",
      "          315768538,\n",
      "          27671014,\n",
      "          175492082,\n",
      "          35770933,\n",
      "          486218399,\n",
      "          33714164,\n",
      "          175492083,\n",
      "          23509718,\n",
      "          122940767,\n",
      "          315958203,\n",
      "          122940759,\n",
      "          231207530,\n",
      "          231207529,\n",
      "          122940762,\n",
      "          122940765,\n",
      "          404866355,\n",
      "          31541082,\n",
      "          175493538,\n",
      "          140616428,\n",
      "          327423978,\n",
      "          122942800,\n",
      "          404866358,\n",
      "          31541083,\n",
      "          36366059,\n",
      "          27655578,\n",
      "          27655576,\n",
      "          140614750,\n",
      "          326764022,\n",
      "          392281784,\n",
      "          36361034,\n",
      "          161683259,\n",
      "          36375747,\n",
      "          161683274,\n",
      "          122932236,\n",
      "          36359307,\n",
      "          175498551]}\n",
      "{'nodes': [534672451],\n",
      " 'osmid': 158881,\n",
      " 'relations': [],\n",
      " 'tags': [{'key': u'addr:postcode', 'value': u'31820'},\n",
      "          {'key': u'admin_level', 'value': u'8'},\n",
      "          {'key': u'boundary', 'value': u'administrative'},\n",
      "          {'key': u'name', 'value': u'Pibrac'},\n",
      "          {'key': u'name:fr', 'value': u'Pibrac'},\n",
      "          {'key': u'name:ru',\n",
      "           'value': u'\\u041f\\u0438\\u0431\\u0440\\u0430\\u043a'},\n",
      "          {'key': u'name:uk',\n",
      "           'value': u'\\u041f\\u0456\\u0431\\u0440\\u0430\\u043a'},\n",
      "          {'key': u'name:zh', 'value': u'\\u76ae\\u5e03\\u62c9\\u514b'},\n",
      "          {'key': u'population', 'value': 8226},\n",
      "          {'key': u'ref:FR:SIREN', 'value': u'213104177'},\n",
      "          {'key': u'ref:INSEE', 'value': u'31417'},\n",
      "          {'key': u'source:population', 'value': 'INSEE 2014'},\n",
      "          {'key': u'type', 'value': u'boundary'},\n",
      "          {'key': u'website', 'value': u'http://www.pibrac.fr/'},\n",
      "          {'key': u'wikidata', 'value': u'Q733833'},\n",
      "          {'key': u'wikipedia', 'value': u'fr:Pibrac'}],\n",
      " 'userid': 94578,\n",
      " 'ways': [34698263,\n",
      "          34698262,\n",
      "          502733016,\n",
      "          502733015,\n",
      "          136915784,\n",
      "          36355019,\n",
      "          167184451,\n",
      "          167184445,\n",
      "          167184447,\n",
      "          167047009,\n",
      "          167047005,\n",
      "          167047007,\n",
      "          167047004,\n",
      "          31433137,\n",
      "          31433138,\n",
      "          36353842,\n",
      "          166581580,\n",
      "          166583220,\n",
      "          166581581,\n",
      "          502733025,\n",
      "          502733024,\n",
      "          36353843]}\n"
     ]
    }
   ],
   "source": [
    "%time inodes, irelations = clean_accuracy(dataset_dict, gold_standard_insee)\n",
    "for inode in inodes:\n",
    "    pprint(dataset_dict[\"nodes\"][inode])\n",
    "for irelation in irelations:\n",
    "    pprint(dataset_dict[\"relations\"][irelation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning completeness issues'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning completeness issues *[cleaning](#Data cleaning)*\n",
    "\n",
    "In data completeness audit [section](#Data completeness) we have indentified missing records in the dataset. It turned out that issues indentified as completeness issues may be seen as accuracy issues since the pharmacies are not  recorded with the proper name.\n",
    "\n",
    "We can deal with these issues with a simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean completeness issues\n",
    "def clean_completeness(dataset, mapping):\n",
    "    \"\"\"\n",
    "    Clean completeness issues according to Pages Jaunes standard.\n",
    "    \n",
    "    The dataset is modified in-place.\n",
    "    \n",
    "    - dataset: dataset to be cleaned\n",
    "    - mapping: mapping of phamarcy names\n",
    "    - return: list of updated nodes index\n",
    "    \"\"\"\n",
    "    nodes = [ ]\n",
    "    for inode, node in enumerate(dataset[\"nodes\"]):\n",
    "        tag_keys = [tag['key'] for tag in node['tags']]\n",
    "        #Has amenity tag?\n",
    "        try:\n",
    "            amenity_index = tag_keys.index('amenity')\n",
    "        except ValueError:\n",
    "            continue\n",
    "        else:\n",
    "            #Check this is a pharmacy\n",
    "            if node[\"tags\"][amenity_index]['value'].lower() != \"pharmacy\":\n",
    "                continue\n",
    "        #Fix pharmacy name\n",
    "        name_index = tag_keys.index('name')\n",
    "        name = node[\"tags\"][name_index][\"value\"]\n",
    "        try:\n",
    "            fixed_name = mapping[name]\n",
    "        except KeyError:\n",
    "            print u\"{} is left unchanged.\".format(name)\n",
    "        else:\n",
    "            node[\"tags\"][name_index][\"value\"] = fixed_name\n",
    "            nodes.append(inode)\n",
    "            print u\"{} has been updated to {}.\".format(name, fixed_name)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pharmacie Saint-Simon is left unchanged.\n",
      "Pharmacie de Lardenne is left unchanged.\n",
      "Pharmacie de Pahin has been updated to Pharmacie De Pahin.\n",
      "Pharmacie du Touch is left unchanged.\n",
      "Pharmacie Bruna Rosso is left unchanged.\n",
      "Pharmacie des Marots is left unchanged.\n",
      "Pharmacie Ãdourad Cabane is left unchanged.\n",
      "Pharmacie de la Paderne is left unchanged.\n",
      "Pharmacie des Jasmins is left unchanged.\n",
      "Pharmacie des Pradettes is left unchanged.\n",
      "Pharmaccie Arc-en-Ciel has been updated to Pharmacie Arc En Ciel.\n",
      "Pharmacie Robin has been updated to Pharmacie Julien RiviÃ©re-Sacaze.\n",
      "Pharmacie RibÃ¨re has been updated to Pharmacie Denise RibÃ¨re.\n",
      "Pharmacie des Tibaous is left unchanged.\n",
      "Pharmacie Hebraud Meneghetti has been updated to La Pharmacie Du Vieux Pigeonnier.\n",
      "Pharmacie de la Commanderie has been updated to Pharmacie De La Commanderie.\n",
      "Pharmacie de la RamÃ©e has been updated to Pharmacie De La RamÃ©e.\n",
      "Pharmacie CAP 2000 has been updated to Pharmacie Cap 2000.\n",
      "Pharmacie Sol is left unchanged.\n",
      "Pharmacie du Centre has been updated to Pharmacie Du Centre.\n",
      "CPU times: user 424 ms, sys: 36 ms, total: 460 ms\n",
      "Wall time: 386 ms\n",
      "{'latitude': 43.5900516,\n",
      " 'longitude': 1.3166181,\n",
      " 'osmid': 496007079,\n",
      " 'tags': [{'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': 'Pharmacie De Pahin'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310015961'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 1626}\n",
      "{'latitude': 43.5874398,\n",
      " 'longitude': 1.3507017,\n",
      " 'osmid': 2157269359,\n",
      " 'tags': [{'key': u'addr:postcode', 'value': u'31170'},\n",
      "          {'key': u'addr:street', 'value': u'Boulevard Vincent Auriol'},\n",
      "          {'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie Arc En Ciel'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310016126'},\n",
      "          {'key': u'source', 'value': u'survey;Celtipharm - 10/2014'}],\n",
      " 'userid': 1626}\n",
      "{'latitude': 43.5856034,\n",
      " 'longitude': 1.3425341,\n",
      " 'osmid': 2345987160,\n",
      " 'tags': [{'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie Julien Rivi\\xe9re-Sacaze'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310016043'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 1626}\n",
      "{'latitude': 43.5769559,\n",
      " 'longitude': 1.3272867,\n",
      " 'osmid': 2638335519,\n",
      " 'tags': [{'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie Denise Rib\\xe8re'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310016076'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 1626}\n",
      "{'latitude': 43.5878392,\n",
      " 'longitude': 1.359229,\n",
      " 'osmid': 3480190235,\n",
      " 'tags': [{'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'La Pharmacie Du Vieux Pigeonnier'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310019609'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 1897475}\n",
      "{'latitude': 43.5758931,\n",
      " 'longitude': 1.3484491,\n",
      " 'osmid': 3481661049,\n",
      " 'tags': [{'key': u'addr:housenumber', 'value': u'110'},\n",
      "          {'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie De La Commanderie'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310016027'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 107257}\n",
      "{'latitude': 43.5604804,\n",
      " 'longitude': 1.3461474,\n",
      " 'osmid': 3481665294,\n",
      " 'tags': [{'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie De La Ram\\xe9e'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310019401'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 1626}\n",
      "{'latitude': 43.5753067,\n",
      " 'longitude': 1.3186646,\n",
      " 'osmid': 3482415283,\n",
      " 'tags': [{'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie Cap 2000'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310017264'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 1626}\n",
      "{'latitude': 43.5669856,\n",
      " 'longitude': 1.296668,\n",
      " 'osmid': 3482512267,\n",
      " 'tags': [{'key': u'addr:city', 'value': u'Plaisance-du-Touch'},\n",
      "          {'key': u'addr:housenumber', 'value': u'6'},\n",
      "          {'key': u'addr:postcode', 'value': u'31830'},\n",
      "          {'key': u'addr:street', 'value': u'Place Fr\\xe9d\\xe9ric Bombail'},\n",
      "          {'key': u'amenity', 'value': u'pharmacy'},\n",
      "          {'key': u'description', 'value': u'B\\xe2timent C'},\n",
      "          {'key': u'dispensing', 'value': u'yes'},\n",
      "          {'key': u'name', 'value': u'Pharmacie Du Centre'},\n",
      "          {'key': u'ref:FR:FINESS', 'value': u'310010244'},\n",
      "          {'key': u'source', 'value': u'Celtipharm - 10/2014'}],\n",
      " 'userid': 567956}\n"
     ]
    }
   ],
   "source": [
    "pharmacy_mapping = {u\"Pharmacie RibÃ¨re\": u\"Pharmacie Denise RibÃ¨re\", \n",
    "                    u\"Pharmaccie Arc-en-Ciel\": u\"Pharmacie Arc En Ciel\", \n",
    "                    u\"Pharmacie Robin\": u\"Pharmacie Julien RiviÃ©re-Sacaze\",\n",
    "                    u\"Pharmacie Hebraud Meneghetti\": u\"La Pharmacie Du Vieux Pigeonnier\",\n",
    "                    u\"Pharmacie de la RamÃ©e\": u\"Pharmacie De La RamÃ©e\",\n",
    "                    u\"Pharmacie de la Commanderie\": u\"Pharmacie De La Commanderie\",\n",
    "                    u\"Pharmacie du Centre\": u\"Pharmacie Du Centre\",\n",
    "                    u\"Pharmacie de Pahin\": \"Pharmacie De Pahin\",\n",
    "                    u\"Pharmacie CAP 2000\": u\"Pharmacie Cap 2000\"}\n",
    "%time inodes = clean_completeness(dataset_dict, pharmacy_mapping)\n",
    "for inode in inodes:\n",
    "    pprint(dataset_dict[\"nodes\"][inode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning consistency issues'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning consistency issues *[cleaning](#Data cleaning)*\n",
    "\n",
    "Some **node**s, **way**s or **relation**s are referenced in **way** or **relation** items but are missing in dataset we have extracted. I have decided to remove any of those items from the dictionnary-like dataset in order to keep a consistent database as output.\n",
    "\n",
    "The audit class can be requested to get a set of missing items. Knowing the list, we just have to remove any reference to those items in our cleaned dataset. This can be done with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean consistency issues\n",
    "def clean_consistency(dataset, missing_nodes, missing_ways, missing_relations):\n",
    "    \"\"\"\n",
    "    Clean consistency issues by removing any reference to missing nodes, ways or relations.\n",
    "    \n",
    "    The dataset is modified in-place.\n",
    "    \n",
    "    - dataset: dataset to be cleaned\n",
    "    - missing_nodes: set of missing nodes\n",
    "    - missing_ways: set of missing ways\n",
    "    - missing_relations: set of missing relations\n",
    "    - return: set of updated ways index, set of updated relations\n",
    "    \"\"\"\n",
    "    ways = set()\n",
    "    relations = set()\n",
    "    #Process ways\n",
    "    for iway, way in enumerate(dataset[\"ways\"]):\n",
    "        to_be_removed = [ ]\n",
    "        for inode, node in enumerate(way[\"nodes\"]):\n",
    "            if node in missing_nodes:\n",
    "                to_be_removed.append(inode)\n",
    "        to_be_removed.reverse() #so we do not change indexes of items in list while we iterate\n",
    "        for i in to_be_removed:\n",
    "            value = way[\"nodes\"].pop(i)\n",
    "            assert value in missing_nodes #cross-check, just in case I introduce errors by mistake\n",
    "            ways.add(iway)\n",
    "    print \"{} ways updated.\".format(len(ways))\n",
    "    #Process relations\n",
    "    for irelation, relation in enumerate(dataset[\"relations\"]):\n",
    "        for (kind, missing) in [(\"nodes\", missing_nodes), (\"ways\", missing_ways), (\"relations\", missing_relations)]:\n",
    "            to_be_removed = [ ]\n",
    "            for iitem, item in enumerate(relation[kind]):\n",
    "                if item in missing:\n",
    "                    to_be_removed.append(iitem)\n",
    "            to_be_removed.reverse() #so we do not change indexes of items in list while we iterate\n",
    "            for i in to_be_removed:\n",
    "                value = relation[kind].pop(i)\n",
    "                assert value in missing #cross-check, just in case I introduce errors by mistake\n",
    "                relations.add(irelation)\n",
    "    print \"{} relations updated.\".format(len(relations))\n",
    "    return ways, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635 ways updated.\n",
      "145 relations updated.\n",
      "CPU times: user 96 ms, sys: 24 ms, total: 120 ms\n",
      "Wall time: 75.9 ms\n"
     ]
    }
   ],
   "source": [
    "missing_nodes = full_audit[3].getMissingNodes()\n",
    "missing_ways = full_audit[3].getMissingWays()\n",
    "missing_relations = full_audit[3].getMissingRelations()\n",
    "%time iways, irelations = clean_consistency(dataset_dict, missing_nodes, missing_ways, missing_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of updated ways and relations matches the number of issues we have identified earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning uniformity issues'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean uniformity issues *[cleaning](#Data cleaning)*\n",
    "\n",
    "In data quality audit [section](#Data uniformity) we have identified two different kinds of nonconformities:\n",
    "- missing addr:housenumber or addr:postcode\n",
    "- non-uniform way of naming streets component\n",
    "\n",
    "We will not fix the addr:housenumber because there is no obvious way to do it. We'll try to fix addr:postcode as we may have postcode information in city data. We may not be able to fix 100% of postcode though:\n",
    "- some big cities have multiple postcodes\n",
    "- the city in addr:city may not match the city name\n",
    "\n",
    "Finally, we can easilly solve the second kind of non-uniformity by providing a mapping.\n",
    "\n",
    "The following function performs all cleaning related to uniformity issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean uniformity issues\n",
    "def clean_uniformity(dataset, street_mapping):\n",
    "    \"\"\"\n",
    "    Clean uniformity issues by:\n",
    "    - replacing street components by uniform ones\n",
    "    - trying to set postcode automatically\n",
    "    \n",
    "    The dataset is modified in-place.\n",
    "    \n",
    "    - dataset: dataset to be cleaned\n",
    "    - street_mapping: mapping of street components\n",
    "    - return: set of updated nodes, set of updated ways index, set of updated relations\n",
    "    \"\"\"\n",
    "    nodes = set()\n",
    "    ways = set()\n",
    "    relations = set()\n",
    "    #Process street components\n",
    "    def process_street_component(kind, dataset, mapping, updated):\n",
    "        for iitem, item in enumerate(dataset[kind]):\n",
    "            for tag in item['tags']:\n",
    "                if tag['key'] == 'addr:street':    \n",
    "                    component = tag['value'].strip().split()[0]\n",
    "                    try:\n",
    "                        fixed_component = mapping[component]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    else:\n",
    "                        tag['value'] = tag['value'].replace(component, fixed_component, 1)\n",
    "                        updated.add(iitem)\n",
    "    for kind, mapping, updated in [(\"nodes\", street_mapping, nodes),\n",
    "                                   (\"ways\", street_mapping, ways),\n",
    "                                   (\"relations\", street_mapping, relations)]:\n",
    "        process_street_component(kind, dataset, mapping, updated)\n",
    "    #Process post codes\n",
    "    #First we need to get a mapping of city with postcode\n",
    "    city_postcodes = { }\n",
    "    def get_postcodes(kind, dataset, postcodes):\n",
    "        for iitem, item in enumerate(dataset[kind]):\n",
    "            tag_keys = [tag['key'] for tag in item['tags']]\n",
    "            try:\n",
    "                population_index = tag_keys.index('ref:INSEE')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            else:\n",
    "                #This is a city\n",
    "                try:\n",
    "                    city = item['tags'][tag_keys.index('name:fr')]['value']\n",
    "                except ValueError:\n",
    "                    city = item['tags'][tag_keys.index('name')]['value']\n",
    "                #Get postcode\n",
    "                try:\n",
    "                    postcode = item['tags'][tag_keys.index('addr:postcode')]['value']\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    if postcode.isdigit():\n",
    "                        postcodes[city] = postcode\n",
    "                    else:\n",
    "                        print \"Items in {} cannot be fixed because there are many postcodes: {}.\".format(\n",
    "                                city, postcode)\n",
    "    for kind in [\"nodes\", \"relations\"]:\n",
    "        get_postcodes(kind, dataset, city_postcodes)\n",
    "    #Now we can try to fix addresses for which postcode is missing\n",
    "    def update_postcodes(kind, dataset, postcodes, updated):\n",
    "        for iitem, item in enumerate(dataset[kind]):\n",
    "            tag_keys = [tag['key'] for tag in item['tags']]\n",
    "            try:\n",
    "                street_index = tag_keys.index('addr:street')\n",
    "            except ValueError:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    postcode_index = tag_keys.index('addr:postcode')\n",
    "                except ValueError:\n",
    "                    #postcode is not set\n",
    "                    try:\n",
    "                        city = item[\"tags\"][tag_keys.index('addr:city')][\"value\"]\n",
    "                    except ValueError:\n",
    "                        #Item has neither postcode nor city...\n",
    "                        continue\n",
    "                    try:\n",
    "                        postcode = postcodes[city]\n",
    "                    except KeyError:\n",
    "                        #We have no postcode for this city\n",
    "                        #print \"{} with osmid {} cannot be fixed because there is no postcode for city {}.\".format(\n",
    "                        #        kind, item['osmid'], city)\n",
    "                        continue\n",
    "                    else:\n",
    "                        item[\"tags\"].append({\"key\": \"addr:postcode\", \"value\": postcode})\n",
    "                        #print \"{} with osmid {} now has a addr:postcode {}.\".format(\n",
    "                        #        kind, item['osmid'], postcode)\n",
    "                        updated.add(iitem)\n",
    "                else:\n",
    "                    #postcode is already set\n",
    "                    continue\n",
    "    for kind, updated in [(\"nodes\", nodes),\n",
    "                          (\"ways\", ways),\n",
    "                          (\"relations\", relations)]:\n",
    "        update_postcodes(kind, dataset, city_postcodes, updated)   \n",
    "    print \"{} nodes updated.\".format(len(nodes))\n",
    "    print \"{} ways updated.\".format(len(ways))\n",
    "    print \"{} relations updated.\".format(len(relations))    \n",
    "    return nodes, ways, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'Boulevard', u'Rond-Point', u'rue', u'impasse', u'avenue', u'Route', u'Passage', u'Impasse', u'Av.', u'Chemin', u'place', u'Clos', u'Place', u'all\\xe9e', u'Rue', u'Avenue', u'All\\xe9e'])\n"
     ]
    }
   ],
   "source": [
    "#Reminder of all patterns encountered\n",
    "print streets_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in Toulouse cannot be fixed because there are many postcodes: 31000;31100;31200;31300;31400;31500.\n",
      "590 nodes updated.\n",
      "2 ways updated.\n",
      "0 relations updated.\n",
      "CPU times: user 1.01 s, sys: 44 ms, total: 1.06 s\n",
      "Wall time: 945 ms\n"
     ]
    }
   ],
   "source": [
    "#Mapping\n",
    "street_mapping = {u\"rue\": u\"Rue\",\n",
    "                  u\"impasse\": u\"Impasse\",\n",
    "                  u\"avenue\": u\"Avenue\",\n",
    "                  u\"Av.\": u\"Avenue\",\n",
    "                  u\"place\": u\"Place\",\n",
    "                  u\"allÃ©e\": u\"AllÃ©e\"}\n",
    "%time inodes, iways, irelations = clean_uniformity(dataset_dict, street_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been able to fix few postcodes issues (all for which city was provided and was different from Toulouse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Cleaning conclusion'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion *[cleaning](#Data cleaning)*\n",
    "\n",
    "Cleaning is far from being perfect, but we have illustrated different techniques to clean a dataset. This is definitely a time-consumming activity :)\n",
    "\n",
    "It's now time to export dataset into files and databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data export'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data export *[top](#Top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='JSON MongoDB'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To JSON and MongoDB *[export](#Data export)*\n",
    "\n",
    "This is the couple I am least confortable with. Luckily, our dictionnary-like structure is really adapted to both transformation to JSON file or for mass import into MongoDB.\n",
    "\n",
    "Let's start with dumping a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 824 ms, sys: 104 ms, total: 928 ms\n",
      "Wall time: 950 ms\n",
      "Size of JSON file 68692676 bytes.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('data.json', 'w') as fobj:\n",
    "    %time fobj.write(json.dumps(dataset_dict))\n",
    "print \"Size of JSON file {} bytes.\".format(os.path.getsize('data.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing is pretty fast for a 68 Mb file.\n",
    "\n",
    "Let's try to reload the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 s, sys: 92 ms, total: 2.16 s\n",
      "Wall time: 2.17 s\n",
      "Dataset with 428787 nodes, 71960 ways and 469 relations.\n"
     ]
    }
   ],
   "source": [
    "with open('data.json', 'r') as fobj:\n",
    "    %time dataset_dict = json.loads(fobj.read())\n",
    "print \"Dataset with {} nodes, {} ways and {} relations.\".format(len(dataset_dict[\"nodes\"]), \n",
    "        len(dataset_dict[\"ways\"]), len(dataset_dict[\"relations\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not lost anything. Reading is a JSON is longer than writing it though. We can now store the data into a MongoDB database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Connect to MongoDB and remove any previous database (if any)\n",
    "from pymongo import MongoClient\n",
    "mongodb_client = MongoClient()\n",
    "mongodb_client.drop_database('udacity-wrangling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'nodes', u'system.indexes', u'ways', u'relations']\n"
     ]
    }
   ],
   "source": [
    "#Mass import from JSON file of documents\n",
    "db = mongodb_client['udacity-wrangling']\n",
    "nodes = db['nodes']\n",
    "nodes.insert_many(dataset_dict[\"nodes\"])\n",
    "ways = db['ways']\n",
    "ways.insert_many(dataset_dict[\"ways\"])\n",
    "relations = db[\"relations\"]\n",
    "relations.insert_many(dataset_dict[\"relations\"])\n",
    "print db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'osmid': 8138771, u'tags': [{u'value': u'traffic_signals', u'key': u'highway'}], u'userid': 134812, u'longitude': 1.3904169, u'latitude': 43.5618389, u'_id': ObjectId('59a0bb83c8f807671d8c09aa')}\n",
      "{u'nodes': [26554082, 26554083, 26554084, 26554085, 26554086, 26554087, 26554088, 26554089, 26554090, 26554091, 26554092, 26554093, 26554094, 492014327, 492014294, 26554096, 26554097, 492005005, 492005006, 26554098, 26554099, 26554100, 26554101, 26554102, 26554103, 1824910001, 1824910003, 26554104, 1824910007, 26554105, 1824910010, 26554106, 26554107, 26554108, 26554079, 26554080, 26554082], u'_id': ObjectId('59a0bb8ac8f807671d92949d'), u'userid': 125897, u'osmid': 4357819, u'tags': []}\n",
      "{u'osmid': 13551, u'tags': [{u'value': u'Rue des Catalpas', u'key': u'name'}, {u'value': u'associatedStreet', u'key': u'type'}], u'ways': [6228254], u'userid': 722137, u'relations': [], u'nodes': [265545746, 265545747, 265545748, 265546434, 265546435, 265546436], u'_id': ObjectId('59a0bb8bc8f807671d93adb5')}\n"
     ]
    }
   ],
   "source": [
    "#What's in there?\n",
    "print nodes.find_one()\n",
    "print ways.find_one()\n",
    "print relations.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb83c8f807671d8c09aa'),\n",
      " u'latitude': 43.5618389,\n",
      " u'longitude': 1.3904169,\n",
      " u'osmid': 8138771,\n",
      " u'tags': [{u'key': u'highway', u'value': u'traffic_signals'}],\n",
      " u'userid': 134812}\n"
     ]
    }
   ],
   "source": [
    "#Request by OpenStreetMap id:\n",
    "for item in nodes.find({'osmid': 8138771}):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb83c8f807671d8c09aa'),\n",
      " u'latitude': 43.5618389,\n",
      " u'longitude': 1.3904169,\n",
      " u'osmid': 8138771,\n",
      " u'tags': [{u'key': u'highway', u'value': u'traffic_signals'}],\n",
      " u'userid': 134812}\n",
      "{u'_id': ObjectId('59a0bb83c8f807671d8c09ab'),\n",
      " u'latitude': 43.5619922,\n",
      " u'longitude': 1.3901005,\n",
      " u'osmid': 8138772,\n",
      " u'tags': [],\n",
      " u'userid': 134812}\n",
      "{u'_id': ObjectId('59a0bb83c8f807671d8c0c9c'),\n",
      " u'latitude': 43.5702514,\n",
      " u'longitude': 1.3904678,\n",
      " u'osmid': 53869616,\n",
      " u'tags': [],\n",
      " u'userid': 13384}\n"
     ]
    }
   ],
   "source": [
    "#Request east-most nodes (max 3 nodes are returned), SQL LIMIT equivalent\n",
    "for item in nodes.find({'longitude': {'$gt': 1.39}}).limit(3):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb83c8f807671d8c0a9b'),\n",
      " u'latitude': 43.5827846,\n",
      " u'longitude': 1.3466543,\n",
      " u'osmid': 26691412,\n",
      " u'tags': [{u'key': u'addr:postcode', u'value': u'31170'},\n",
      "           {u'key': u'name', u'value': u'Tournefeuille'},\n",
      "           {u'key': u'name:fr', u'value': u'Tournefeuille'},\n",
      "           {u'key': u'name:oc', u'value': u'Tornafu\\xe8lha'},\n",
      "           {u'key': u'place', u'value': u'town'},\n",
      "           {u'key': u'population', u'value': u'26674'},\n",
      "           {u'key': u'ref:FR:SIREN', u'value': u'213105570'},\n",
      "           {u'key': u'ref:INSEE', u'value': u'31557'},\n",
      "           {u'key': u'source:population', u'value': u'INSEE 2014'},\n",
      "           {u'key': u'wikidata', u'value': u'Q328022'},\n",
      "           {u'key': u'wikipedia', u'value': u'fr:Tournefeuille'}],\n",
      " u'userid': 6523296}\n",
      "{u'_id': ObjectId('59a0bb83c8f807671d8c0b9b'),\n",
      " u'latitude': 43.582037,\n",
      " u'longitude': 1.3450115,\n",
      " u'osmid': 51983918,\n",
      " u'tags': [],\n",
      " u'userid': 148173}\n",
      "{u'_id': ObjectId('59a0bb83c8f807671d8c0ba1'),\n",
      " u'latitude': 43.582536,\n",
      " u'longitude': 1.3440651,\n",
      " u'osmid': 51983992,\n",
      " u'tags': [],\n",
      " u'userid': 148173}\n"
     ]
    }
   ],
   "source": [
    "#Refined latitude / longitude box (equivalent to City center dataset)\n",
    "for item in nodes.find({'longitude': {'$gt': 1.3434, '$lt': 1.3496}, \n",
    "                        'latitude': {'$gt': 43.5799, '$lt': 43.5838}}).limit(3):\n",
    "    pprint(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb8bc8f807671d93adb5'),\n",
      " u'nodes': [265545746, 265545747, 265545748, 265546434, 265546435, 265546436],\n",
      " u'osmid': 13551,\n",
      " u'relations': [],\n",
      " u'tags': [{u'key': u'name', u'value': u'Rue des Catalpas'},\n",
      "           {u'key': u'type', u'value': u'associatedStreet'}],\n",
      " u'userid': 722137,\n",
      " u'ways': [6228254]}\n"
     ]
    }
   ],
   "source": [
    "#Find in document attributes (list)\n",
    "for item in relations.find({\"nodes\": 265545746}):\n",
    "    pprint(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'name:fr': u'Tournefeuille',\n",
      " u'population': u'26674',\n",
      " u'ref:INSEE': u'31557',\n",
      " u'source:population': u'INSEE 2014'}\n",
      "{u'name:fr': u'Plaisance-du-Touch',\n",
      " u'population': 17278,\n",
      " u'ref:INSEE': u'31424',\n",
      " u'source:population': u'INSEE 2014'}\n",
      "{u'name:fr': u'Colomiers',\n",
      " u'population': 38541,\n",
      " u'ref:INSEE': u'31149',\n",
      " u'source:population': u'INSEE 2014'}\n",
      "{u'name:fr': u'Toulouse',\n",
      " u'population': 466297,\n",
      " u'ref:INSEE': u'31555',\n",
      " u'source:population': u'INSEE 2014'}\n",
      "{u'name:fr': u'Pibrac',\n",
      " u'population': 8226,\n",
      " u'ref:INSEE': u'31417',\n",
      " u'source:population': u'INSEE 2014'}\n"
     ]
    }
   ],
   "source": [
    "#Find in document attributes (dict) with kind of SQL UNION and $and operator:\n",
    "filter_tags = lambda x: x[\"key\"] in ('name:fr', 'ref:INSEE', 'population', 'source:population')\n",
    "city_criteria = {\"$and\": [{\"tags.key\": \"ref:INSEE\"}, {\"tags.key\": \"population\"}]}\n",
    "items = [node for node in nodes.find(city_criteria)]\n",
    "items.extend([relation for relation in relations.find(city_criteria)])\n",
    "for item in items:\n",
    "    pprint(dict((t[\"key\"], t[\"value\"]) for t in filter(filter_tags, item[\"tags\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb86c8f807671d91f5af'),\n",
      " u'latitude': 43.5874398,\n",
      " u'longitude': 1.3507017,\n",
      " u'osmid': 2157269359L,\n",
      " u'tags': [{u'key': u'addr:postcode', u'value': u'31170'},\n",
      "           {u'key': u'addr:street', u'value': u'Boulevard Vincent Auriol'},\n",
      "           {u'key': u'amenity', u'value': u'pharmacy'},\n",
      "           {u'key': u'dispensing', u'value': u'yes'},\n",
      "           {u'key': u'name', u'value': u'Pharmacie Arc En Ciel'},\n",
      "           {u'key': u'ref:FR:FINESS', u'value': u'310016126'},\n",
      "           {u'key': u'source', u'value': u'survey;Celtipharm - 10/2014'}],\n",
      " u'userid': 1626}\n"
     ]
    }
   ],
   "source": [
    "#Look for pharmacies in Tournefeuille either with city name or postcode, combination of and and or operators:\n",
    "find_criteria = {\"$and\": [{\"tags.key\": \"amenity\", \"tags.value\": \"pharmacy\"}, \n",
    "                          {\"$or\": [{\"tags.key\": \"addr:postcode\", \"tags.value\": \"31170\"},\n",
    "                                   {\"tags.key\": \"addr:city\", \"tags.value\": \"Tournefeuille\"}]}]}\n",
    "for node in nodes.find(find_criteria):\n",
    "    pprint(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a single one pharmacy with either addr:postcode or addr:city attribute set for pharmacies in Tournfeuille.\n",
    "The cleaning made is insufficiant because we haven't been able to affect postcode or city to nodes missing both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 68.14245767712174, u'_id': 1685}\n",
      "{u'count': 8.911184340943171, u'_id': 306172}\n",
      "{u'count': 4.423175142903119, u'_id': 115737}\n",
      "{u'count': 2.2099550592718527, u'_id': 148173}\n",
      "{u'count': 1.865961421405033, u'_id': 1573648}\n",
      "{u'count': 1.806491334858566, u'_id': 13384}\n",
      "{u'count': 1.1768080655430317, u'_id': 125897}\n",
      "{u'count': 0.980906603978199, u'_id': 571625}\n",
      "{u'count': 0.9356626949977495, u'_id': 1626}\n",
      "{u'count': 0.9316980225613183, u'_id': 297482}\n",
      "428787\n"
     ]
    }
   ],
   "source": [
    "#Get major contributors: usage of aggregation, grouping and sorting (descending)\n",
    "#We need to build an aggregation pipeline\n",
    "for item in nodes.aggregate([{\"$group\": {\"_id\": \"$userid\", \"count\": {\"$sum\": 1}}}, #group by userid and count\n",
    "                             {\"$project\": {\"count\": { \"$multiply\": [ \"$count\", 100. / nodes.count()]}}}, # calculate %\n",
    "                             {\"$sort\": {\"count\": -1}}, #sort by descending order\n",
    "                             {\"$limit\": 10}]): #limit to 10 users\n",
    "    print item\n",
    "print nodes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user owning the most number of nodes in OpenStreetMap is also owner of more than 68% of all nodes ! Let's find out if we can know more about him:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb8ac8f807671d929aee'),\n",
      " u'nodes': [343600511,\n",
      "            3222241389L,\n",
      "            343600512,\n",
      "            343600507,\n",
      "            3222241387L,\n",
      "            343600513,\n",
      "            3222241386L,\n",
      "            247420475],\n",
      " u'osmid': 30907996,\n",
      " u'tags': [{u'key': u'highway', u'value': u'tertiary'},\n",
      "           {u'key': u'junction', u'value': u'roundabout'},\n",
      "           {u'key': u'name', u'value': u'Rond-Point Ut\\xe9bo'},\n",
      "           {u'key': u'source',\n",
      "            u'value': u'cadastre-dgi-fr source : Direction G\\xe9n\\xe9rale des Imp\\xf4ts - Cadastre. Mise \\xe0 jour : 2009'}],\n",
      " u'userid': 1685}\n"
     ]
    }
   ],
   "source": [
    "#More information about userid 1685\n",
    "for item in ways.find({\"userid\": 1685, \"tags.key\": \"source\"}).limit(1):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that he is working in Direction GÃ©nÃ©rale des ImpÃ´ts (French Tax Directorate) in the land registry office (french word: cadastre). French land registry office seems to use OpenStreetMap ;)\n",
    "\n",
    "To end this MongoDB capabilities overview, we are trying to do a join to get latitude and longitude of all nodes in the previous way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': ObjectId('59a0bb8ac8f807671d929aee'),\n",
      " u'nodes': 343600511,\n",
      " u'osmid': 30907996,\n",
      " u'tags': [{u'key': u'highway', u'value': u'tertiary'},\n",
      "           {u'key': u'junction', u'value': u'roundabout'},\n",
      "           {u'key': u'name', u'value': u'Rond-Point Ut\\xe9bo'},\n",
      "           {u'key': u'source',\n",
      "            u'value': u'cadastre-dgi-fr source : Direction G\\xe9n\\xe9rale des Imp\\xf4ts - Cadastre. Mise \\xe0 jour : 2009'}],\n",
      " u'userid': 1685}\n",
      "{u'_id': ObjectId('59a0bb8ac8f807671d929aee'),\n",
      " u'nodes': 3222241389L,\n",
      " u'osmid': 30907996,\n",
      " u'tags': [{u'key': u'highway', u'value': u'tertiary'},\n",
      "           {u'key': u'junction', u'value': u'roundabout'},\n",
      "           {u'key': u'name', u'value': u'Rond-Point Ut\\xe9bo'},\n",
      "           {u'key': u'source',\n",
      "            u'value': u'cadastre-dgi-fr source : Direction G\\xe9n\\xe9rale des Imp\\xf4ts - Cadastre. Mise \\xe0 jour : 2009'}],\n",
      " u'userid': 1685}\n",
      "{u'_id': ObjectId('59a0bb8ac8f807671d929aee'),\n",
      " u'nodes': 343600512,\n",
      " u'osmid': 30907996,\n",
      " u'tags': [{u'key': u'highway', u'value': u'tertiary'},\n",
      "           {u'key': u'junction', u'value': u'roundabout'},\n",
      "           {u'key': u'name', u'value': u'Rond-Point Ut\\xe9bo'},\n",
      "           {u'key': u'source',\n",
      "            u'value': u'cadastre-dgi-fr source : Direction G\\xe9n\\xe9rale des Imp\\xf4ts - Cadastre. Mise \\xe0 jour : 2009'}],\n",
      " u'userid': 1685}\n"
     ]
    }
   ],
   "source": [
    "#First let's have a look at what unwind operator does. Match operator enables to use a find in an aggregation\n",
    "for item in ways.aggregate([{\"$match\": {\"osmid\": 30907996}},\n",
    "                            {\"$unwind\": \"$nodes\"},\n",
    "                            {\"$limit\": 3}]):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unwind operator enables to \"deconstruct\" an array. In fact we are going to need this operator to perform a join (lookup operator). Lookup performs a left join. The join syntax is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your version of Mongodb does not support $lookup operator.\n"
     ]
    }
   ],
   "source": [
    "#Now let's join\n",
    "if map(lambda x: int(x), mongodb_client.server_info()['version'].split('.')) > (3, 2, 0):\n",
    "    try:\n",
    "        for item in ways.aggregate([{\"$match\": {\"osmid\": 30907996}},\n",
    "                                    {\"$unwind\": \"$nodes\"},\n",
    "                                    {\"$lookup\": {\"$from\": \"nodes\", \"$localField\": \"nodes\", \"$foreignField\": \"osmid\"}}]):\n",
    "            pprint(item)\n",
    "    except:\n",
    "        \"This code is untested because I don't have a recent version of MongoDB yet, I need to updgrade.\"\n",
    "else:\n",
    "    print \"Your version of Mongodb does not support $lookup operator.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a LTS version of Ubuntu 16.04 coming with MongoDB 2.6 not supporting yet $lookup operator (supported from version 3.2).\n",
    "\n",
    "MongoDB differs from what I know about SQL:\n",
    "- no need of schema, it's very esay to store collections from Python\n",
    "- aggregate, group, sort, limit, etc... works a bit differently than SQL equivalent but product is well documented and it's easy to find answers on [StackOverflow](https://stackoverflow.com/questions/tagged/mongodb) (as always...)\n",
    "- \"documents\" are also easier to read, information is not spread into multiple tables\n",
    "- joining is a bit trickier than SQL, but that's probably the price to pay for lack of strict schema and \"completeness\" of records. I definitely need to install a newer version to take advantage of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='csv SQLite'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To csv and SQLite *[export](#Data export)*\n",
    "\n",
    "I have not re-writen another SAX content handler for csv export. I am just going to make use of dictionnary-like structure to export csv files matching the way I am going to structure the SQL database.\n",
    "\n",
    "I will create the following files:\n",
    "\n",
    "| File                    | Description                                          |\n",
    "|:----------------------- |:---------------------------------------------------- |\n",
    "| nodes.csv               | nodes attributes                                     |\n",
    "| nodes_tags.csv          | nodes tags                                           |\n",
    "| ways.csv                | ways attributes                                      |\n",
    "| ways_nodes.csv          | references to nodes from ways                        |\n",
    "| ways_tags.csv           | ways tags                                            |\n",
    "| relations.csv           | relations attributes                                 |\n",
    "| relations_nodes.csv     | references to nodes from relations                   |\n",
    "| relations_ways.csv      | references to ways from relations                    |\n",
    "| relations_relations.csv | references to relations from relations               |\n",
    "\n",
    "Contrary to MongoDB, preparation for mass import required more work !\n",
    "\n",
    "The following function will create all those files. It takes the dictionnary-like structure as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def export_to_csv(dataset):\n",
    "    \"\"\"\n",
    "    Export dataset to csv files.\n",
    "    \n",
    "    WARNING: the dataset is modified in-place in the operation.\n",
    "    Python csv module does not support Unicode strings, we need to to encode manually in UTF-8.\n",
    "    The encoding is done by function.\n",
    "    \n",
    "    - dataset_dict: dataset to be exported.\n",
    "    \"\"\"        \n",
    "    def encode_utf8(dct):\n",
    "        \"\"\"\n",
    "        Encode keys and values in UTF-8 if they are unicode strings.\n",
    "        \n",
    "        - dct: input dictionnary with possibly unicode strings for text keys and values.\n",
    "        - return: dictionnary with text keys and values into byte strings encoded in UTF-8.\n",
    "        \"\"\"\n",
    "        output = { }\n",
    "        for key, value in dct.iteritems():\n",
    "            if type(key) == type(u\"\"):\n",
    "                key = key.encode('utf-8')\n",
    "            if type(value) == type(u\"\"):\n",
    "                value = value.encode('utf-8')\n",
    "            output[key] = value\n",
    "        return output\n",
    "    \n",
    "    #Process nodes\n",
    "    with open('nodes.csv', 'w') as f_nodes, open('nodes_tags.csv', 'w') as f_tags:\n",
    "        f_nodes_fields = ['osmid', 'longitude', 'latitude', 'userid']\n",
    "        f_tags_fields = ['node_id', 'key', 'value']\n",
    "        w_nodes = csv.DictWriter(f_nodes, fieldnames=f_nodes_fields)\n",
    "        w_tags = csv.DictWriter(f_tags, fieldnames=f_tags_fields)\n",
    "        w_nodes.writeheader()\n",
    "        w_tags.writeheader()\n",
    "        for node in dataset[\"nodes\"]:\n",
    "            osmid = node['osmid']\n",
    "            tags = node.pop(\"tags\")\n",
    "            for tag in tags:\n",
    "                tag[u'node_id'] = osmid\n",
    "                w_tags.writerow(encode_utf8(tag))\n",
    "            w_nodes.writerow(encode_utf8(node))\n",
    "    \n",
    "    #Process ways\n",
    "    with open('ways.csv', 'w') as f_ways, open('ways_nodes.csv', 'w') as f_nodes, \\\n",
    "            open('ways_tags.csv', 'w') as f_tags:\n",
    "        f_ways_fields = ['osmid', 'userid']\n",
    "        f_nodes_fields = ['way_id', 'node_id']\n",
    "        f_tags_fields = ['way_id', 'key', 'value']\n",
    "        w_ways = csv.DictWriter(f_ways, fieldnames=f_ways_fields)\n",
    "        w_nodes = csv.DictWriter(f_nodes, fieldnames=f_nodes_fields)\n",
    "        w_tags = csv.DictWriter(f_tags, fieldnames=f_tags_fields)\n",
    "        w_ways.writeheader()\n",
    "        w_nodes.writeheader()\n",
    "        w_tags.writeheader()\n",
    "        for way in dataset[\"ways\"]:\n",
    "            osmid = way[\"osmid\"]\n",
    "            tags = way.pop(\"tags\")\n",
    "            nodes = way.pop(\"nodes\")\n",
    "            for tag in tags:\n",
    "                tag[u'way_id'] = osmid\n",
    "                w_tags.writerow(encode_utf8(tag))\n",
    "            for node in nodes:\n",
    "                w_nodes.writerow({'node_id': node, 'way_id': osmid})\n",
    "            w_ways.writerow(encode_utf8(way))\n",
    "        \n",
    "    #Process relations\n",
    "    with open('relations_ways.csv', 'w') as f_ways, open('relations_nodes.csv', 'w') as f_nodes,  \\\n",
    "            open('relations_tags.csv', 'w') as f_tags, open('relations.csv', 'w') as f_relations, \\\n",
    "            open('relations_relations.csv', 'w') as f_rel_rel:\n",
    "        f_relations_fields = ['osmid', 'userid']\n",
    "        f_tags_fields = ['relation_id', 'key', 'value']\n",
    "        f_ways_fields = ['relation_id', 'way_id']\n",
    "        f_nodes_fields = ['relation_id', 'node_id']\n",
    "        f_rel_rel_fields = ['relation_container', 'relation_content']\n",
    "        w_relations = csv.DictWriter(f_relations, fieldnames=f_relations_fields)\n",
    "        w_tags = csv.DictWriter(f_tags, fieldnames=f_tags_fields)\n",
    "        w_ways = csv.DictWriter(f_ways, fieldnames=f_ways_fields)\n",
    "        w_nodes = csv.DictWriter(f_nodes, fieldnames=f_nodes_fields)\n",
    "        w_rel_rel = csv.DictWriter(f_rel_rel, fieldnames=f_rel_rel_fields)\n",
    "        w_relations.writeheader()\n",
    "        w_ways.writeheader()\n",
    "        w_nodes.writeheader()\n",
    "        w_tags.writeheader()\n",
    "        w_rel_rel.writeheader()\n",
    "        for relation in dataset[\"relations\"]:\n",
    "            osmid = relation[\"osmid\"]\n",
    "            tags = relation.pop(\"tags\")\n",
    "            nodes = relation.pop(\"nodes\")\n",
    "            ways = relation.pop(\"ways\")\n",
    "            relations = relation.pop(\"relations\")\n",
    "            for tag in tags:\n",
    "                tag[u'relation_id'] = osmid\n",
    "                w_tags.writerow(encode_utf8(tag))\n",
    "            for node in nodes:\n",
    "                w_nodes.writerow({'node_id': node, 'relation_id': osmid})\n",
    "            for way in ways:\n",
    "                w_ways.writerow({'way_id': way, 'relation_id': osmid})\n",
    "            for rel in relations:\n",
    "                w_rel_rel.writerow({'relation_container': osmid, 'relation_content': rel})\n",
    "            w_relations.writerow(encode_utf8(relation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.55 s, sys: 48 ms, total: 4.6 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "#Reload a dataset from JSON (the one we have is linked to MongoDB)\n",
    "with open('data.json', 'r') as fobj:\n",
    "    dataset_csv = json.loads(fobj.read())\n",
    "\n",
    "#Export to csv\n",
    "%time export_to_csv(dataset_csv)\n",
    "\n",
    "#Clean dataset\n",
    "del dataset_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to create the SQLite database. We need to define a schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_schema = \"\"\"\n",
    "CREATE TABLE nodes (\n",
    "    osmid INTEGER PRIMARY KEY NOT NULL,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    userid INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE nodes_tags (\n",
    "    node_id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE ways (\n",
    "    osmid INTEGER PRIMARY KEY NOT NULL,\n",
    "    userid INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_tags (\n",
    "    way_id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE ways_nodes (\n",
    "    way_id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE relations (\n",
    "    osmid INTEGER PRIMARY KEY NOT NULL,\n",
    "    userid INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE relations_tags (\n",
    "    relation_id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE relations_nodes (\n",
    "    relation_id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE relations_ways (\n",
    "    relation_id INTEGER NOT NULL,\n",
    "    way_id INTEGER NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE relations_relations (\n",
    "    relation_container INTEGER NOT NULL,\n",
    "    relation_content INTEGER NOT NULL\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the database\n",
    "import sqlite3\n",
    "#Delete any previous version of database:\n",
    "try:\n",
    "    os.remove('data.sql')\n",
    "except OSError:\n",
    "    pass\n",
    "#Close any previous connection\n",
    "try:\n",
    "    sql_client.close()\n",
    "except NameError:\n",
    "    pass\n",
    "sql_client = sqlite3.connect('data.sql')\n",
    "cursor = sql_client.cursor()\n",
    "cursor.executescript(sql_schema)\n",
    "sql_client.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing nodes.csv [16052487 bytes]... done\n",
      "Importing nodes_tags.csv [1381735 bytes]... done\n",
      "Importing ways.csv [1230356 bytes]... done\n",
      "Importing ways_tags.csv [9720022 bytes]... done\n",
      "Importing ways_nodes.csv [12196915 bytes]... done\n",
      "Importing relations.csv [7282 bytes]... done\n",
      "Importing relations_tags.csv [75737 bytes]... done\n",
      "Importing relations_nodes.csv [153563 bytes]... done\n",
      "Importing relations_ways.csv [168978 bytes]... done\n",
      "Importing relations_relations.csv [274 bytes]... done\n",
      "Database data.sql ready [35205120 bytes].\n"
     ]
    }
   ],
   "source": [
    "#Mass import time (inspired by https://stackoverflow.com/a/2888042/8500344)\n",
    "for table, columns in [('nodes', ['osmid', 'latitude', 'longitude', 'userid']),\n",
    "                       ('nodes_tags', ['node_id', 'key', 'value']),\n",
    "                       ('ways', ['osmid', 'userid']),\n",
    "                       ('ways_tags', ['way_id', 'key', 'value']),\n",
    "                       ('ways_nodes', ['way_id', 'node_id']),\n",
    "                       ('relations', ['osmid', 'userid']),\n",
    "                       ('relations_tags', ['relation_id', 'key', 'value']),\n",
    "                       ('relations_nodes', ['relation_id', 'node_id']),\n",
    "                       ('relations_ways', ['relation_id', 'way_id']),\n",
    "                       ('relations_relations', ['relation_container', 'relation_content'])]:\n",
    "    with open('{}.csv'.format(table)) as fobj:\n",
    "        print \"Importing {}.csv [{} bytes]...\".format(table, os.path.getsize(\"{}.csv\".format(table))),\n",
    "        reader = csv.DictReader(fobj)\n",
    "        script = \"INSERT INTO {} ({}) VALUES ({})\".format(\n",
    "            table, \",\".join(columns), \",\".join(map(lambda x: '?', columns)))\n",
    "        to_db = [[row[col].decode('utf-8') for col in columns] for row in reader]\n",
    "        cursor.executemany(script, to_db)\n",
    "        print \"done\"\n",
    "print \"Database {} ready [{} bytes].\".format('data.sql', os.path.getsize(\"data.sql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the database has been populated with our dataset we can make some requests. Note that the SQLite file is around 35 Mb, smaller than the JSON file (which was 68 Mb).\n",
    "\n",
    "Here are few requests with SQL database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8138771, 43.5618389, 1.3904169, 134812)]\n"
     ]
    }
   ],
   "source": [
    "#Request by OpenStreetMap id:\n",
    "cursor.execute(\"SELECT * FROM nodes WHERE osmid = ?\", (8138771,))\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8138771, 43.5618389, 1.3904169, 134812),\n",
      " (8138772, 43.5619922, 1.3901005, 134812),\n",
      " (53869616, 43.5702514, 1.3904678, 13384)]\n"
     ]
    }
   ],
   "source": [
    "#Request east-most nodes (max 3 nodes are returned)\n",
    "cursor.execute(\"SELECT * FROM nodes WHERE longitude > 1.39 LIMIT 3\")\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26691412, 43.5827846, 1.3466543, 6523296),\n",
      " (51983918, 43.582037, 1.3450115, 148173),\n",
      " (51983992, 43.582536, 1.3440651, 148173)]\n"
     ]
    }
   ],
   "source": [
    "#Refined latitude / longitude box (equivalent to City center dataset)\n",
    "cursor.execute(\"\"\"SELECT * FROM nodes \n",
    "               WHERE longitude > ? AND longitude < ? AND \n",
    "                     latitude > ? AND latitude < ?\n",
    "               LIMIT 3\"\"\", (1.3434, 1.3496, 43.5799, 43.5838))\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13551, 722137)]\n"
     ]
    }
   ],
   "source": [
    "#Find in list attributes (contrary to MongoDB we need a join here)\n",
    "cursor.execute(\"\"\"SELECT relations.* FROM relations\n",
    "               JOIN relations_nodes ON relations_nodes.relation_id = relations.osmid\n",
    "               JOIN nodes ON relations_nodes.node_id = nodes.osmid\n",
    "               WHERE nodes.osmid = ?\"\"\", (265545746,))\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26691412, u'name:fr', u'Tournefeuille'),\n",
      " (26691412, u'population', u'26674'),\n",
      " (26691412, u'ref:INSEE', u'31557'),\n",
      " (26691412, u'source:population', u'INSEE 2014'),\n",
      " (26691742, u'name:fr', u'Plaisance-du-Touch'),\n",
      " (26691742, u'population', u'17278'),\n",
      " (26691742, u'ref:INSEE', u'31424'),\n",
      " (26691742, u'source:population', u'INSEE 2014')]\n"
     ]
    }
   ],
   "source": [
    "#Find in dict attributes\n",
    "cursor.execute(\"\"\"SELECT cities.node, nodes_tags.key, nodes_tags.value\n",
    "                  FROM nodes_tags\n",
    "                  JOIN                                                     --- This is a comment\n",
    "                   (SELECT nodes.osmid AS node FROM nodes                  --- This is a subquery getting nodes\n",
    "                    JOIN nodes_tags ON nodes_tags.node_id = nodes.osmid    --- with tags ref:INSEE and population\n",
    "                    WHERE nodes_tags.key = ? OR nodes_tags.key = ?         --- we cannot use AND here so instead\n",
    "                    GROUP BY nodes.osmid                                   --- we use GROUP BY, count() and HAVING\n",
    "                    HAVING count(*) = 2) cities ON cities.node = nodes_tags.node_id\n",
    "                  WHERE nodes_tags.key IN (?, ?, ?, ?)\"\"\", \n",
    "               (u\"ref:INSEE\", u\"population\", u\"ref:INSEE\", u\"population\", u\"source:population\", u\"name:fr\"))\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases (SQL and MongoDB), making requests based to tag keys and values requires relatively complex code.\n",
    "Having a list of tags key / value is higly flexible because new tags may be added easilly but the cost to pay is to make queries less easy. We could have selected a subset of tags we are interested in and turn them into fields in the database.\n",
    "\n",
    "Looking for pharmacies in Tournefeuille would require the a similar kind of request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1685, 68.14245767712174),\n",
      " (306172, 8.911184340943173),\n",
      " (115737, 4.42317514290312),\n",
      " (148173, 2.209955059271853),\n",
      " (1573648, 1.8659614214050333),\n",
      " (13384, 1.806491334858566),\n",
      " (125897, 1.1768080655430317),\n",
      " (571625, 0.980906603978199),\n",
      " (1626, 0.9356626949977495),\n",
      " (297482, 0.9316980225613183)]\n"
     ]
    }
   ],
   "source": [
    "#Get major contributors - we can make arithmetics in SQL requests\n",
    "cursor.execute(\"\"\"SELECT userid, count(*) * 100. / (SELECT count(*) FROM nodes) as n \n",
    "                  FROM nodes GROUP BY userid ORDER BY n DESC LIMIT 10\"\"\")\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'source',\n",
      "  u'cadastre-dgi-fr source : Direction G\\xe9n\\xe9rale des Imp\\xf4ts - Cadastre. Mise \\xe0 jour : 2014')]\n"
     ]
    }
   ],
   "source": [
    "#More info on user 1685\n",
    "#cursor.execute(\"\"\"SELECT ways_tags.key, ways_tags.value \"\"\")\n",
    "cursor.execute(\"\"\"SELECT ways_tags.key, ways_tags.value \n",
    "                  FROM ways_tags\n",
    "                  JOIN ways ON ways.osmid = ways_tags.way_id\n",
    "                  WHERE ways.userid = ?\n",
    "                  GROUP BY ways.userid\"\"\", (1685,))\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the latest MongoDB request requiring a join is simple in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(343600511,),\n",
      " (3222241389,),\n",
      " (343600512,),\n",
      " (343600507,),\n",
      " (3222241387,),\n",
      " (343600513,),\n",
      " (3222241386,),\n",
      " (247420475,)]\n"
     ]
    }
   ],
   "source": [
    "#Now let's join\n",
    "cursor.execute(\"\"\"SELECT nodes.osmid FROM nodes\n",
    "                  JOIN ways_nodes ON ways_nodes.node_id = nodes.osmid\n",
    "                  WHERE ways_nodes.way_id = ?\"\"\", (30907996,))\n",
    "pprint(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We are done with the database\n",
    "sql_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated earlier than joining in MongoDB was more difficult (moreover it's only supported from version 3.2), but actually SQL requires more join operations due to the desing of tables.\n",
    "\n",
    "In both cases, it is not straightforward to write requests with tags. The way OpenStreetMap uses tags is very flexible but this flexibility is also playing against consistency in an open-source database (we have found a lot of non-uniformities in the way addresses are recorded for example).\n",
    "\n",
    "As we have kept a schema very close from the one in OpenStreetMap, (both for SQLite and MongoDB), we face the same difficulties: the way tags are recorded brings maximum flexibility but writing request becomes also much more difficult. There is then a trade-off to make between a database structure which enables maximum flexibility (and by extension a higher number of applications) or a database more strict in terms of tagging (fixed-tags for example) that would greatly improve the simplicity of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Conclusion'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions *[top](#Top)*\n",
    "\n",
    "Udacity instructors said thad many data analysts reports spending most of their time (up to around 75%) wrangling data, at the end of this project I can say that I now better understand that statement :)\n",
    "\n",
    "Data wrangling is also a with a lot of trade-off:\n",
    "\n",
    "- We can probably spend infinite amount of time auditing and cleaning large datasets. The more time is spent on wrangling and the less time will be passed on data exploration tasks. But also the more potential applications we can have with the dataset.\n",
    "\n",
    "- The way dataset is made persistant also plays an important role. Schema may be strict and improve the ease of use during data exploration but also decreases the potential of reusability for different purposes.\n",
    "\n",
    "In terms of techniques, there are a lot of improvements we can make:\n",
    "\n",
    "- The dataset of the project remains very small. Even the 1 Gb dataset is small and can fit into memory. So I haven't really explored all techniques for dealing with large datasets: if I used SAX XML parsers, I also, at some point, had a Python dictionnary with the whole dataset in memory. JSON writing also requires to have a full dataset in memory. I gave a brief try to shelve and diskcache modules but I found them very slow, probably a misuse.\n",
    "\n",
    "- String comparison may fail, due to different case, accented characters or not, use of - in names,... We could probably increase the flexibility of cleaning (and auting) steps by using some fuzzy comparisons. In addition,  caution is needed to properly deal with unicode strings, encoding and decoding ([Ned Batchelder talk at Pycon 2012 is worth the detour.](https://www.youtube.com/watch?v=sgHbC6udIqc))\n",
    "\n",
    "- Finally, my first experience in SQL helps but is not enough when it comes to bigger requests (with subrequests, pivoting...) so I need more practice. MongoDB is the first no-SQL database I use, so I need to practice even more with aggregation pipelines and lookups once I have updated my configuration. I have also seen that it exists a lot of Object Relational Mapping systems based on Python and Mongodb. This would probably worth a try when I have some spare time :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Appendix\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix *[top](#Top)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[OpenStreetData wiki](http://wiki.openstreetmap.org/wiki/Main_Page)<hr>\n",
    "[INSEE](https://www.insee.fr/en/accueil) is French National Institute of Statistics and Economic Information. In this project, it is used as *gold* standard.<hr>\n",
    "[Pages Jaunes](https://www.pagesjaunes.fr/annuaire/tournefeuille-31/pharmacies) used as another *gold* standard.<hr>\n",
    "Validating XML tree with [XML Schema](https://www.w3schools.com/xml/schema_intro.asp) can be done with [lxml](http://lxml.de/validation.html) library. This technique has not been used here as the structure of XML is simple enough. Additionaly, XML Schema validation requires to have XML data into memory and may not be suitable for large files like the ones we might have here.<hr>\n",
    "Get line number in a content handler with SAX parser on [StackOverflow](https://stackoverflow.com/a/15477803/8500344).<hr>\n",
    "Display lists as html tables in notebook on [StackOverflow](https://stackoverflow.com/a/42323522/8500344).<hr>\n",
    "Fuzzy string matching blog post on [streamhacker.com](https://streamhacker.com/2011/10/31/fuzzy-string-matching-python/).<hr>\n",
    "MongoDB from Python: [pymongo](http://api.mongodb.com/python/current/tutorial.html) tutorial.<hr>\n",
    "MongoDB [manual](https://docs.mongodb.com/manual/).<hr>\n",
    "Geometry algorithms for [inclusion](http://geomalgorithms.com/a03-_inclusion.html) checks.<hr>\n",
    "StackOverflow for [MongoDB](https://stackoverflow.com/questions/tagged/mongodb).<hr>\n",
    "Ned Batchelder talk at Pycon 2012 on [YouTube](https://www.youtube.com/watch?v=sgHbC6udIqc): How to stop unicode pain ?<hr>\n",
    "Python SQLite import from csv on [StackOverflow](https://stackoverflow.com/a/2888042/8500344).<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataAnalysis]",
   "language": "python",
   "name": "conda-env-DataAnalysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
